{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eligibility traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install -U gymnasium pygame moviepy swig\n",
    "    !pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rng = np.random.default_rng()\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# os.environ[\"IMAGEIO_FFMPEG_EXE\"] = \"/opt/homebrew/bin/ffmpeg\" # if moviepy complains about not finding ffmpeg, put its path here\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import gymnasium as gym\n",
    "print(\"gym version:\", gym.__version__)\n",
    "\n",
    "import pygame\n",
    "from moviepy.editor import ImageSequenceClip, ipython_display\n",
    "\n",
    "class GymRecorder(object):\n",
    "    \"\"\"\n",
    "    Simple wrapper over moviepy to generate a .gif with the frames of a gym environment.\n",
    "    \n",
    "    The environment must have the render_mode `rgb_array_list`.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self._frames = []\n",
    "\n",
    "    def record(self, frames):\n",
    "        \"To be called at the end of an episode.\"\n",
    "        for frame in frames:\n",
    "            self._frames.append(np.array(frame))\n",
    "\n",
    "    def make_video(self, filename):\n",
    "        \"Generates the gif video.\"\n",
    "        directory = os.path.dirname(os.path.abspath(filename))\n",
    "        if not os.path.exists(directory):\n",
    "            os.mkdir(directory)\n",
    "        self.clip = ImageSequenceClip(list(self._frames), fps=self.env.metadata[\"render_fps\"])\n",
    "        self.clip.write_gif(filename, fps=self.env.metadata[\"render_fps\"], loop=0)\n",
    "        del self._frames\n",
    "        self._frames = []\n",
    "\n",
    "def running_average(x, N):\n",
    "    kernel = np.ones(N) / N\n",
    "    return np.convolve(x, kernel, mode='same')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning in Gridworld\n",
    "\n",
    "### Random interaction with the environment\n",
    "\n",
    "The goal of this exercise is to solve the **Gridworld** problem using Q-learning. The code is adapted from  <https://gymnasium.farama.org/tutorials/environment_creation/>\n",
    "\n",
    "The agent is represented by the blue circle: the **state** $s$ of the agent is its position in the 5x5 grid, i.e. a number between 0 and 24.\n",
    "\n",
    "The agent can move either to the left, right, top or bottom. When the agent tries to move outside of the environment, it stays at its current position. There are four **actions** $a$ available, which are deterministic.    \n",
    "\n",
    "Its goal is to reach the green circle, while avoiding the red ones. Actions leading to the green circle receive a reward $r$ of +100, actions leading to a red square receive a reward of -100. The episode ends in those states. All other actions have a reward of -1. An episode stops after 100 steps if a goal has not been reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\", \"rgb_array_list\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, render_mode=None, size=5, rewards=[100, -100, -1]):\n",
    "        self.size = size  # The size of the square grid\n",
    "        self.window_size = 512  # The size of the PyGame window\n",
    "        self.rewards = rewards\n",
    "        self._step = 0\n",
    "\n",
    "        # The state is the flattened (x, y) coordinate of the agent\n",
    "        self.observation_space = gym.spaces.Discrete(size**2)\n",
    "\n",
    "        # Goal location\n",
    "        self._target_location = np.array([3, 2], dtype=int)\n",
    "        self._distractor1_location = np.array([3, 1], dtype=int)\n",
    "        self._distractor2_location = np.array([2, 2], dtype=int)\n",
    "\n",
    "        # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]), # right\n",
    "            1: np.array([0, 1]), # down\n",
    "            2: np.array([-1, 0]), # left\n",
    "            3: np.array([0, -1]), # up\n",
    "        }\n",
    "\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        if self.render_mode == \"rgb_array_list\":\n",
    "            self._frames = []\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        self.font = pygame.font.SysFont(None, 16)\n",
    "        self.Q = np.zeros((self.observation_space.n, self.action_space.n))\n",
    "\n",
    "\n",
    "    def _state2coordinates(self, state):\n",
    "        \"Returns coordinates of a state.\"\n",
    "        return (state % self.size, int(state/self.size))\n",
    "\n",
    "    def _coordinate2state(self, coord):\n",
    "        \"Returns the state with the coordinates.\"\n",
    "        return coord[1] * self.size + coord[0]\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "\n",
    "        self._step = 0\n",
    "\n",
    "        # Initial location\n",
    "        self._agent_location = np.array([0, 0], dtype=int)\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "        \n",
    "        if self.render_mode == \"rgb_array_list\":\n",
    "            self._frames = []\n",
    "            self._render_frame()\n",
    "\n",
    "        return self._coordinate2state(self._agent_location), {}\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        # Map the action (element of {0,1,2,3}) to the direction we walk in\n",
    "        direction = self._action_to_direction[action]\n",
    "        \n",
    "        # We use `np.clip` to make sure we don't leave the grid\n",
    "        self._agent_location = np.clip(\n",
    "            self._agent_location + direction, 0, self.size - 1\n",
    "        )\n",
    "        \n",
    "        # An episode is done if the agent has reached the target or the distractors\n",
    "        if np.array_equal(self._agent_location, self._target_location):\n",
    "            terminal = True\n",
    "            reward = self.rewards[0]\n",
    "        elif np.array_equal(self._agent_location, self._distractor1_location) \\\n",
    "            or np.array_equal(self._agent_location, self._distractor2_location):\n",
    "            terminal = True\n",
    "            reward = self.rewards[1]\n",
    "        else:\n",
    "            terminal = False\n",
    "            reward = self.rewards[2]\n",
    "\n",
    "        if self.render_mode == \"human\" or self.render_mode == \"rgb_array_list\":\n",
    "            self._render_frame()\n",
    "\n",
    "        self._step += 1\n",
    "        if self._step == 100:\n",
    "            truncated = True\n",
    "        else:\n",
    "            truncated = False\n",
    "\n",
    "        return self._coordinate2state(self._agent_location), reward, terminal, truncated, {}\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return self._render_frame()\n",
    "        elif self.render_mode == \"rgb_array_list\":\n",
    "            f = self._frames.copy()\n",
    "            self._frames = []\n",
    "            return f\n",
    "\n",
    "    def _render_frame(self):\n",
    "\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode(\n",
    "                (self.window_size, self.window_size)\n",
    "            )\n",
    "        if self.clock is None and self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((255, 255, 255))\n",
    "        pix_square_size = (\n",
    "            self.window_size / self.size\n",
    "        )  # The size of a single grid square in pixels\n",
    "\n",
    "        # First we draw the target and the distractors\n",
    "        pygame.draw.rect(\n",
    "            canvas,\n",
    "            (0, 255, 0),\n",
    "            pygame.Rect(\n",
    "                pix_square_size * self._target_location,\n",
    "                (pix_square_size, pix_square_size),\n",
    "            ),\n",
    "        )\n",
    "        pygame.draw.rect(\n",
    "            canvas,\n",
    "            (255, 0, 0),\n",
    "            pygame.Rect(\n",
    "                pix_square_size * self._distractor1_location,\n",
    "                (pix_square_size, pix_square_size),\n",
    "            ),\n",
    "        )\n",
    "        pygame.draw.rect(\n",
    "            canvas,\n",
    "            (255, 0, 0),\n",
    "            pygame.Rect(\n",
    "                pix_square_size * self._distractor2_location,\n",
    "                (pix_square_size, pix_square_size),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Now we draw the agent\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            (0, 0, 255),\n",
    "            (self._agent_location + 0.5) * pix_square_size,\n",
    "            pix_square_size / 3,\n",
    "        )\n",
    "\n",
    "        # Add some gridlines\n",
    "        for x in range(self.size + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (0, pix_square_size * x),\n",
    "                (self.window_size, pix_square_size * x),\n",
    "                width=3,\n",
    "            )\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (pix_square_size * x, 0),\n",
    "                (pix_square_size * x, self.window_size),\n",
    "                width=3,\n",
    "            )\n",
    "\n",
    "        # Print Q-values\n",
    "        for x in range(self.size):\n",
    "            for y in range(self.size):\n",
    "                s = self._coordinate2state((x, y))\n",
    "                \n",
    "                # Up\n",
    "                val = f\"{self.Q[s, 3]:+.2f}\"\n",
    "                text = self.font.render(val, True, (0, 0, 0))\n",
    "                canvas.blit(text, \n",
    "                    ((x + 0.5) * pix_square_size - 6, \n",
    "                    (y) * pix_square_size + 6)\n",
    "                )\n",
    "                # Down\n",
    "                val = f\"{self.Q[s, 1]:+.2f}\"\n",
    "                text = self.font.render(val, True, (0, 0, 0))\n",
    "                canvas.blit(text, \n",
    "                    ((x + 0.5) * pix_square_size - 6, \n",
    "                    (y+1) * pix_square_size - 12)\n",
    "                )\n",
    "                # Left\n",
    "                val = f\"{self.Q[s, 2]:+.2f}\"\n",
    "                text = self.font.render(val, True, (0, 0, 0))\n",
    "                canvas.blit(text, \n",
    "                    ((x) * pix_square_size + 6, \n",
    "                    (y+ 0.5) * pix_square_size - 6)\n",
    "                )\n",
    "                # Right\n",
    "                val = f\"{self.Q[s, 0]:+.2f}\"\n",
    "                text = self.font.render(val, True, (0, 0, 0))\n",
    "                canvas.blit(text, \n",
    "                    ((x + 1) * pix_square_size - 32, \n",
    "                    (y+ 0.5) * pix_square_size - 6)\n",
    "                )\n",
    "\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            # The following line copies our drawings from `canvas` to the visible window\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
    "            # The following line will automatically add a delay to keep the framerate stable.\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "\n",
    "        elif  self.render_mode == \"rgb_array\":\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "        elif  self.render_mode == \"rgb_array_list\":\n",
    "            array = np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "            self._frames.append(array)\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.Q = np.zeros((self.env.observation_space.n, self.env.action_space.n))\n",
    "        \n",
    "    def act(self, state):\n",
    "        \"Selects an action randomly\"\n",
    "        return self.env.action_space.sample()\n",
    "    \n",
    "    def train(self, nb_episodes, recorder=None):\n",
    "        \"Runs the agent on the environment for nb_episodes.\"\n",
    "        # Returns\n",
    "        returns = []\n",
    "        steps = []\n",
    "\n",
    "        # Fixed number of episodes\n",
    "        for episode in range(nb_episodes):\n",
    "\n",
    "            # Reset\n",
    "            state, info = self.env.reset()\n",
    "            done = False\n",
    "            nb_steps = 0\n",
    "\n",
    "            # Store rewards\n",
    "            return_episode = 0.0\n",
    "\n",
    "            # Sample the episode\n",
    "            while not done:\n",
    "                    \n",
    "                # Select an action \n",
    "                action = self.act(state)\n",
    "\n",
    "                # Perform the action\n",
    "                next_state, reward, terminal, truncated, info = self.env.step(action)\n",
    "                \n",
    "                # Append reward\n",
    "                return_episode += reward\n",
    "\n",
    "                # Go in the next state\n",
    "                state = next_state\n",
    "\n",
    "                # Increment time\n",
    "                nb_steps += 1\n",
    "\n",
    "                # Terminal state\n",
    "                done = terminal or truncated\n",
    "                \n",
    "                # Pass the Q table to the GUI\n",
    "                self.env.Q = self.Q  \n",
    "\n",
    "            # Store info\n",
    "            returns.append(return_episode)\n",
    "            steps.append(nb_steps)\n",
    "            \n",
    "        return returns, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = GridWorldEnv(render_mode='human')\n",
    "\n",
    "# Create the agent\n",
    "agent = RandomAgent(env)\n",
    "\n",
    "# Perform random episodes\n",
    "returns, steps = agent.train(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Adapt your Q-learning agent from last exercise to the problem. The main difference is the call to `self.env.Q = self.Q` so that the GUI displays the Q-values, the rest is similar. Train it for 100 episodes with the right hyperparameters and without rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    Q-learning agent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, gamma, exploration, decay, alpha):\n",
    "        \"\"\"\n",
    "        :param env: gym-like environment\n",
    "        :param gamma: discount factor\n",
    "        :param exploration: exploration parameter\n",
    "        :param decay: exploration decay parameter\n",
    "        :param alpha: learning rate\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.exploration = exploration\n",
    "        self.decay = decay\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Q_table\n",
    "        self.Q = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"Returns an action using epsilon-greedy action selection.\"\n",
    "        \n",
    "        action = rng.choice(np.where(self.Q[state, :] == self.Q[state, :].max())[0])\n",
    "        \n",
    "        if rng.random() < self.exploration:\n",
    "            action = self.env.action_space.sample() \n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"Updates the agent using a single transition.\"\n",
    "        \n",
    "        # Bellman target\n",
    "        target = reward\n",
    "        \n",
    "        if not done:\n",
    "            target += self.gamma * self.Q[next_state, :].max()\n",
    "        \n",
    "        # Update the Q-value\n",
    "        self.Q[state, action] += self.alpha * (target - self.Q[state, action])\n",
    "            \n",
    "        # Decay exploration parameter\n",
    "        self.exploration = self.exploration * (1 - self.decay)\n",
    "            \n",
    "    \n",
    "    def train(self, nb_episodes, recorder=None):\n",
    "        \"Runs the agent on the environment for nb_episodes.\"\n",
    "\n",
    "        # Returns\n",
    "        returns = []\n",
    "        steps = []\n",
    "\n",
    "        # Fixed number of episodes\n",
    "        for episode in range(nb_episodes):\n",
    "\n",
    "            # Reset\n",
    "            state, info = self.env.reset()\n",
    "            done = False\n",
    "            nb_steps = 0\n",
    "\n",
    "            # Store rewards\n",
    "            return_episode = 0.0\n",
    "\n",
    "            # Sample the episode\n",
    "            while not done:\n",
    "\n",
    "                # Select an action \n",
    "                action = self.act(state)\n",
    "\n",
    "                # Perform the action\n",
    "                next_state, reward, terminal, truncated, info = self.env.step(action)\n",
    "\n",
    "                # Terminal state\n",
    "                done = terminal or truncated\n",
    "                \n",
    "                # Append reward\n",
    "                return_episode += reward\n",
    "\n",
    "                # Learn from the transition\n",
    "                self.update(state, action, reward, next_state, done)\n",
    "\n",
    "                # Go in the next state\n",
    "                state = next_state\n",
    "\n",
    "                # Increment time\n",
    "                nb_steps += 1\n",
    "                \n",
    "                # Pass the Q table to the GUI\n",
    "                self.env.Q = self.Q\n",
    "\n",
    "            # Store info\n",
    "            returns.append(return_episode)\n",
    "            steps.append(nb_steps)\n",
    "\n",
    "            if recorder is not None:\n",
    "                recorder.record(self.env.render())\n",
    "            \n",
    "        return returns, steps\n",
    "    \n",
    "    def test(self, recorder=None):\n",
    "        \"Performs a test episode without exploration.\"\n",
    "        previous_epsilon = self.epsilon\n",
    "        self.epsilon = 0.0\n",
    "        \n",
    "        # Reset\n",
    "        state, info = self.env.reset()\n",
    "        done = False\n",
    "        nb_steps = 0\n",
    "        return_episode= 0\n",
    "\n",
    "        # Sample the episode\n",
    "        while not done:\n",
    "            action = self.act(state)\n",
    "            next_state, reward, done, info = self.env.step(action)\n",
    "            return_episode += reward\n",
    "            state = next_state\n",
    "            nb_steps += 1\n",
    "            \n",
    "        self.epsilon = previous_epsilon\n",
    "\n",
    "        if recorder is not None:\n",
    "            recorder.record(self.env.render())\n",
    "            \n",
    "        return return_episode, nb_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "gamma = 0.99\n",
    "epsilon = 0.1\n",
    "decay_epsilon = 0\n",
    "alpha = 0.1\n",
    "nb_episodes = 100\n",
    "\n",
    "# Create the environment\n",
    "env = GridWorldEnv(render_mode=None)\n",
    "\n",
    "# Create the agent\n",
    "agent = QLearningAgent(env, gamma, epsilon, decay_epsilon, alpha)\n",
    "\n",
    "# Train the agent\n",
    "returns, steps = agent.train(nb_episodes)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(121)\n",
    "plt.plot(returns)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.subplot(122)\n",
    "plt.plot(steps)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Train a Q-learning agent with rendering on. Observe in particular which Q-values are updated when the agent reaches the target. Is it efficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "gamma = 0.99\n",
    "epsilon = 0.1\n",
    "decay_epsilon = 0\n",
    "alpha = 0.1\n",
    "nb_episodes = 10\n",
    "\n",
    "# Create the environment\n",
    "env = GridWorldEnv(render_mode='human')\n",
    "\n",
    "# Create the agent\n",
    "agent = QLearningAgent(env, gamma, epsilon, decay_epsilon, alpha)\n",
    "\n",
    "# Train the agent\n",
    "returns, steps = agent.train(nb_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Modify your agent so that it uses **softmax action selection**, with a temperature $\\tau = 1.0$ and a suitable decay. What does it change?\n",
    "\n",
    "If you have time, write a generic class for the Q-learning agent where you can select the action selection method flexibly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftQLearningAgent(QLearningAgent):\n",
    "    \"\"\"\n",
    "    Q-learning agent with softmax or e-greedy AS.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, gamma, action_selection, alpha):\n",
    "        \"\"\"\n",
    "        :param env: gym-like environment\n",
    "        :param gamma: discount factor\n",
    "        :param action selection: exploration mechanism\n",
    "        :param alpha: learning rate\n",
    "        \"\"\"\n",
    "        self.action_selection = action_selection\n",
    "        \n",
    "        super().__init__(env, gamma, action_selection['param'], action_selection['decay'], alpha)\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"Returns an action using epsilon-greedy or softmax  action selection.\"\n",
    "            \n",
    "        if self.action_selection['type'] == \"egreedy\":\n",
    "            # epsilon-greedy\n",
    "            if rng.uniform(0, 1, 1) < self.exploration:\n",
    "                action = self.env.action_space.sample() \n",
    "            else:\n",
    "                action = rng.choice(np.where(self.Q[state, :] == self.Q[state, :].max())[0])\n",
    "        else: \n",
    "            # softmax\n",
    "            logits = np.exp((self.Q[state, :] - self.Q[state, :].max())/self.exploration)\n",
    "            probas = logits / np.sum(logits)\n",
    "            action = rng.choice(range(4), p=probas) \n",
    "        \n",
    "        return action\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "gamma = 0.99\n",
    "#action_selection  = {'type': \"egreedy\", \"param\": 0.1, \"decay\": 0.0}\n",
    "action_selection  = {'type': \"softmax\", \"param\": 1.0, \"decay\": 0.0}\n",
    "alpha = 0.1\n",
    "nb_episodes = 100\n",
    "\n",
    "# Create the environment\n",
    "env = GridWorldEnv(render_mode=None)\n",
    "\n",
    "# Create the agent\n",
    "agent = SoftQLearningAgent(env, gamma, action_selection, alpha)\n",
    "\n",
    "# Train the agent \n",
    "returns, steps = agent.train(nb_episodes)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(121)\n",
    "plt.plot(returns)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.subplot(122)\n",
    "plt.plot(steps)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** The agent explores much less at the end of training, as the difference between the Q-values becomes high enough to become greedy. In particular, it quickly stops to go to the red squares. In this environment, there is no real need to decay tau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eligibility traces\n",
    "\n",
    "The main drawback of Q-learning is that it needs many episodes to converge (**sample complexity**).\n",
    "\n",
    "One way to speed up learning is to use eligibility traces, one per state-action pair:\n",
    "\n",
    "```python\n",
    "traces = np.zeros((nb_states, nb_actions))\n",
    "```\n",
    "\n",
    "After each transition $(s_t, a_t)$, Q($\\lambda$) updates a **trace** $e(s_t, a_t)$ and modifies all Q-values as:\n",
    "\n",
    "1.  The trace of the last transition is incremented from 1:\n",
    "    \n",
    "$$e(s_t, a_t) = e(s_t, a_t) +1$$\n",
    "    \n",
    "2. Q($\\lambda$)-learning is applied on **ALL** Q-values, using the TD error at time $t$:\n",
    "    \n",
    "$$Q(s, a) = Q(s, a) + \\alpha \\, (r_{t+1} + \\gamma \\, \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)) \\, e(s, a)$$\n",
    "    \n",
    "3. All traces are exponentially decreased using the trace parameter $\\lambda$ (e.g. 0.7):\n",
    "\n",
    "$$\n",
    "e(s, a) = \\lambda \\, \\gamma \\, e(s, a)\n",
    "$$\n",
    "\n",
    "All traces are reset to 0 at the beginning of an episode.\n",
    "\n",
    "**Q:** Implement eligibility traces in your Q($\\lambda$)-learning agent and see if it improves convergence. Train it with rendering on and observe how all Q-values are updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLambdaLearningAgent(SoftQLearningAgent):\n",
    "    \"\"\"\n",
    "    Q(lambda)-learning agent with softmax or e-greedy AS and eligibility traces.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, gamma, lbda, action_selection, alpha):\n",
    "        \"\"\"\n",
    "        :param env: gym-like environment\n",
    "        :param gamma: discount factor\n",
    "        :param lbda: trace\n",
    "        :param action selection: exploration mechanism\n",
    "        :param alpha: learning rate\n",
    "        \"\"\"\n",
    "        self.lbda = lbda\n",
    "        \n",
    "        # Traces\n",
    "        self.traces = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "        \n",
    "        super().__init__(env, gamma, action_selection, alpha)\n",
    "        \n",
    "\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        # Bellman target\n",
    "        target = reward\n",
    "        if not done:\n",
    "            target += self.gamma * self.Q[next_state, :].max()\n",
    "        \n",
    "        # Update ALL Q-values\n",
    "        self.Q += self.alpha * (target - self.Q[state, action]) * self.traces\n",
    "            \n",
    "        # Decay exploration parameter\n",
    "        self.exploration = self.exploration * (1 - self.decay)\n",
    "        \n",
    "        \n",
    "    def train(self, nb_episodes, recorder=None):\n",
    "        \"Runs the agent on the environment for nb_episodes.\"\n",
    "\n",
    "        # Returns\n",
    "        returns = []\n",
    "        steps = []\n",
    "\n",
    "        # Fixed number of episodes\n",
    "        for episode in range(nb_episodes):\n",
    "\n",
    "            # Reset\n",
    "            state, info = self.env.reset()\n",
    "            done = False\n",
    "            nb_steps = 0\n",
    "            \n",
    "            # Reset traces\n",
    "            self.traces *= 0.0\n",
    "\n",
    "            # Store rewards\n",
    "            return_episode = 0.0\n",
    "\n",
    "            # Sample the episode\n",
    "            while not done:\n",
    "\n",
    "                # Select an action \n",
    "                action = self.act(state)\n",
    "\n",
    "                # Perform the action\n",
    "                next_state, reward, terminal, truncated, info = self.env.step(action)\n",
    "                \n",
    "                # Terminal state\n",
    "                done = terminal or truncated\n",
    "                \n",
    "                # Update return \n",
    "                return_episode += reward\n",
    "                \n",
    "                # Increment trace\n",
    "                self.traces[state, action] += 1\n",
    "\n",
    "                # Learn from the transition\n",
    "                self.update(state, action, reward, next_state, done)\n",
    "                \n",
    "                # Update all traces\n",
    "                self.traces *= self.gamma * self.lbda\n",
    "\n",
    "                # Go in the next state\n",
    "                state = next_state\n",
    "\n",
    "                # Increment time\n",
    "                nb_steps += 1\n",
    "            \n",
    "                \n",
    "                # Pass the Q table to the GUI\n",
    "                self.env.Q = self.Q\n",
    "\n",
    "            # Store info\n",
    "            returns.append(return_episode)\n",
    "            steps.append(nb_steps)\n",
    "\n",
    "            if recorder is not None:\n",
    "                recorder.record(self.env.render())\n",
    "\n",
    "        return returns, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "gamma = 0.99\n",
    "lbda = 0.7\n",
    "#action_selection  = {'type': \"egreedy\", \"param\": 0.1, \"decay\": 0.0}\n",
    "action_selection  = {'type': \"softmax\", \"param\": 1.0, \"decay\": 0.0}\n",
    "alpha = 0.1\n",
    "nb_episodes = 100\n",
    "\n",
    "# Create the environment\n",
    "env = GridWorldEnv()\n",
    "\n",
    "# Create the agent\n",
    "agent = QLambdaLearningAgent(env, gamma, lbda, action_selection, alpha)\n",
    "\n",
    "# Train the agent\n",
    "returns, steps = agent.train(nb_episodes)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(121)\n",
    "plt.plot(returns)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.subplot(122)\n",
    "plt.plot(steps)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "gamma = 0.99\n",
    "lbda = 0.7\n",
    "#action_selection  = {'type': \"egreedy\", \"param\": 0.1, \"decay\": 0.0}\n",
    "action_selection  = {'type': \"softmax\", \"param\": 1.0, \"decay\": 0.0}\n",
    "alpha = 0.1\n",
    "nb_episodes = 10\n",
    "\n",
    "# Create the environment\n",
    "env = GridWorldEnv(render_mode='human')\n",
    "\n",
    "# Create the agent\n",
    "agent = QLambdaLearningAgent(env, gamma, lbda, action_selection, alpha)\n",
    "\n",
    "# Train the agent\n",
    "returns = agent.train(nb_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Vary the trace parameter $\\lambda$ and discuss its influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "gamma = 0.99\n",
    "#action_selection  = {'type': \"egreedy\", \"param\": 0.1, \"decay\": 0.0}\n",
    "action_selection  = {'type': \"softmax\", \"param\": 1.0, \"decay\": 0.0}\n",
    "alpha = 0.1\n",
    "nb_episodes = 100\n",
    "\n",
    "list_returns = []\n",
    "\n",
    "for lbda in np.linspace(0.1, 1.0, 10):\n",
    "\n",
    "    # Create the environment\n",
    "    env = GridWorldEnv()\n",
    "\n",
    "    # Create the agent\n",
    "    agent = QLambdaLearningAgent(env, gamma, lbda, action_selection, alpha)\n",
    "\n",
    "    # Train the agent\n",
    "    returns, steps = agent.train(nb_episodes)\n",
    "    \n",
    "    list_returns.append(returns)\n",
    "    \n",
    "plt.figure(figsize=(12, 6))\n",
    "for idx, lbda in enumerate(np.linspace(0.1, 1.0, 10)):\n",
    "    plt.plot(list_returns[idx], label=str(lbda))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** $\\lambda$ should not be too high nor too low in order to speed up learning. It controls the bias/variance trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Increase the size of Gridworld to 100x100 and observe how long it takes to learn the optimal strategy using eligibility traces or not.\n",
    "\n",
    "```python\n",
    "env = GridWorldEnv(size=100)\n",
    "```\n",
    "\n",
    "Comment on the **curse of dimensionality** and the interest of tabular RL for complex tasks with large state spaces and sparse rewards (e.g. robotics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "gamma = 0.99\n",
    "lbda = 0.7\n",
    "#action_selection  = {'type': \"egreedy\", \"param\": 0.1, \"decay\": 0.0}\n",
    "action_selection  = {'type': \"softmax\", \"param\": 1.0, \"decay\": 0.0}\n",
    "alpha = 0.1\n",
    "nb_episodes = 100\n",
    "\n",
    "# Create the environment\n",
    "env = GridWorldEnv(size=100)\n",
    "\n",
    "# Create the agent\n",
    "agent = SoftQLearningAgent(env, gamma, action_selection, alpha)\n",
    "#agent = QLambdaLearningAgent(env, gamma, lbda, action_selection, alpha)\n",
    "\n",
    "# Train the agent\n",
    "returns, steps = agent.train(nb_episodes)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(121)\n",
    "plt.plot(returns)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.subplot(122)\n",
    "plt.plot(steps)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** When the Gridworld is too big, the likelihood to hit the target per chance when exploring is very low. There are a lot of unsuccessful trials before learning starts to occur. But it happens after a while."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tianshou",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
