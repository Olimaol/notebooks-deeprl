{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3210f163",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Olimaol/notebooks-deeprl/blob/main/solutions/04-Bandits.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will investigate the properties of the action selection schemes seen in the lecture and compare their properties:\n",
    "\n",
    "1. greedy action selection\n",
    "2. $\\epsilon$-greedy action selection\n",
    "3. softmax action selection\n",
    "\n",
    "Let's re-use the definitions of the last exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit:\n",
    "    \"\"\"\n",
    "    n-armed bandit.\n",
    "    \"\"\"\n",
    "    def __init__(self, nb_actions, mean=0.0, std_Q=1.0, std_r=1.0):\n",
    "        \"\"\"\n",
    "        :param nb_actions: number of arms.\n",
    "        :param mean: mean of the normal distribution for $Q^*$.\n",
    "        :param std_Q: standard deviation of the normal distribution for $Q^*$.\n",
    "        :param std_r: standard deviation of the normal distribution for the sampled rewards.\n",
    "        \"\"\"\n",
    "        # Store parameters\n",
    "        self.nb_actions = nb_actions\n",
    "        self.mean = mean\n",
    "        self.std_Q = std_Q\n",
    "        self.std_r = std_r\n",
    "        \n",
    "        # Initialize the true Q-values\n",
    "        self.Q_star = rng.normal(self.mean, self.std_Q, self.nb_actions)\n",
    "        \n",
    "        # Optimal action\n",
    "        self.a_star = self.Q_star.argmax()\n",
    "        \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Sampled a single reward from the bandit.\n",
    "        \n",
    "        :param action: the selected action.\n",
    "        :return: a reward.\n",
    "        \"\"\"\n",
    "        return float(rng.normal(self.Q_star[action], self.std_r, 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_actions = 5\n",
    "bandit = Bandit(nb_actions)\n",
    "\n",
    "all_rewards = []\n",
    "for t in range(1000):\n",
    "    rewards = []\n",
    "    for a in range(nb_actions):\n",
    "        rewards.append(bandit.step(a))\n",
    "    all_rewards.append(rewards)\n",
    "    \n",
    "mean_reward = np.mean(all_rewards, axis=0)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(131)\n",
    "plt.bar(range(nb_actions), bandit.Q_star)\n",
    "plt.xlabel(\"Actions\")\n",
    "plt.ylabel(\"$Q^*(a)$\")\n",
    "plt.subplot(132)\n",
    "plt.bar(range(nb_actions), mean_reward)\n",
    "plt.xlabel(\"Actions\")\n",
    "plt.ylabel(\"$Q_t(a)$\")\n",
    "plt.subplot(133)\n",
    "plt.bar(range(nb_actions), np.abs(bandit.Q_star - mean_reward))\n",
    "plt.xlabel(\"Actions\")\n",
    "plt.ylabel(\"Absolute error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy action selection\n",
    "\n",
    "In **greedy action selection**, we systematically chose the action with the highest estimated Q-value at each play (or randomly when there are ties):\n",
    "\n",
    "$$a_t = \\text{argmax}_a Q_t(a)$$\n",
    "\n",
    "We maintain estimates $Q_t$ of the action values (initialized to 0) using the online formula:\n",
    "\n",
    "$$Q_{t+1}(a_t) = Q_t(a_t) + \\alpha \\, (r_{t} - Q_t(a_t))$$\n",
    "\n",
    "when receiving the sampled reward $r_t$ after taking the action $a_t$. The learning rate $\\alpha$ can be set to 0.1 at first.\n",
    "\n",
    "The algorithm simply alternates between these two steps for 1000 plays (or steps): take an action, update its Q-value. \n",
    "\n",
    "**Q:** Implement the greedy algorithm on the 5-armed bandit.\n",
    "\n",
    "Your algorithm will look like this:\n",
    "\n",
    "* Create a 5-armed bandit (mean of zero, variance of 1).\n",
    "* Initialize the estimated Q-values to 0 with an array of the same size as the bandit.\n",
    "* **for** 1000 plays:\n",
    "    * Select the greedy action $a_t^*$ using the current estimates.\n",
    "    * Sample a reward from $\\mathcal{N}(Q^*(a_t^*), 1)$.\n",
    "    * Update the estimated Q-value of the action taken.\n",
    "    \n",
    "Additionally, you will store the received rewards at each step in an initially empty list or a numpy array of the correct size and plot it in the end. You will also plot the true Q-values and the estimated Q-values at the end of the 1000 plays. \n",
    "\n",
    "*Tip:* to implement the argmax, do not rely on `np.argmax()`. If there are ties in the array, for example at the beginning:\n",
    "\n",
    "```python\n",
    "x = np.array([0, 0, 0, 0, 0])\n",
    "```\n",
    "\n",
    "`x.argmax()` will return you the **first occurrence** of the maximum 0.0 of the array. In this case it will be the index 0, so you will always select the action 0 first. \n",
    "\n",
    "It is much more efficient to retrieve the indices of **all** maxima and randomly select one of them:\n",
    "\n",
    "```python\n",
    "a = rng.choice(np.where(x == x.max())[0])\n",
    "```\n",
    "\n",
    "`np.where(x == x.max())` returns a list of indices where `x` is maximum. `rng.choice()` randomly selects one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "# Learning rate\n",
    "alpha = 0.1\n",
    "\n",
    "# Bandit\n",
    "bandit = Bandit(nb_actions)\n",
    "\n",
    "# Estimates\n",
    "Q_t = np.zeros(nb_actions)\n",
    "\n",
    "# Store the rewards after each step\n",
    "rewards = []\n",
    "\n",
    "# For 1000 plays\n",
    "for step in range(1000):\n",
    "    \n",
    "    # Select the action greedily w.r.t Q_t\n",
    "    action = rng.choice(np.where(Q_t == Q_t.max())[0])\n",
    "    \n",
    "    # Sample the reward\n",
    "    reward = bandit.step(action)\n",
    "    \n",
    "    # Store the received reward\n",
    "    rewards.append(reward)\n",
    "    \n",
    "    # Update the Q-value estimate of the action\n",
    "    Q_t[action] += alpha * (reward - Q_t[action])\n",
    "    \n",
    "# Plot the Q-values and the evolution of rewards\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(131)\n",
    "plt.bar(range(nb_actions), bandit.Q_star)\n",
    "plt.xlabel(\"Actions\")\n",
    "plt.ylabel(\"$Q^*(a)$\")\n",
    "plt.subplot(132)\n",
    "plt.bar(range(nb_actions), Q_t)\n",
    "plt.xlabel(\"Actions\")\n",
    "plt.ylabel(\"$Q_t(a)$\")\n",
    "plt.subplot(133)\n",
    "plt.plot(rewards)\n",
    "plt.xlabel(\"Plays\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Re-run your algorithm multiple times with different values of $Q^*$ (simply recreate the `Bandit`) and observe:\n",
    "\n",
    "1. How much reward you get.\n",
    "2. How your estimated Q-values in the end differ from the true Q-values.\n",
    "3. Whether greedy action action selection finds the optimal action or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** The plot with rewards is very noisy, you do not really see whether you have learned something because of the randomness of the rewards. More often than not, greedy action selection finds the optimal action, or least a not-that-bad action. The estimates `Q_t` have however nothing to see with the true Q-values, as you quickly select the same action and never update the other ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going further, let's turn the agent into a class for better reusability. \n",
    "\n",
    "**Q:** Create a `GreedyAgent` class taking the bandit as an argument as well as the learning rate `alpha=0.1`:\n",
    "\n",
    "```python\n",
    "bandit = Bandit(nb_actions)\n",
    "\n",
    "agent = GreedyAgent(bandit, alpha=0.1)\n",
    "```\n",
    "\n",
    "The constructor should initialize the array of estimated Q-values `Q_t` and store it as an attribute.\n",
    "\n",
    "Define a method `act(self)` that returns the index of the greedy action based on the current estimates, as well as a method `update(self, action, reward)` that allows to update the estimated Q-value of the action given the obtained reward. Define also a `train(self, nb_steps)` method that implements the complete training process for `nb_steps=1000` plays and returns the list of obtained rewards.\n",
    "\n",
    "```python\n",
    "class GreedyAgent:\n",
    "    def __init__(self, bandit, alpha):\n",
    "        # TODO\n",
    "        \n",
    "    def act(self):      \n",
    "        action = # TODO\n",
    "        return action\n",
    "        \n",
    "    def update(self, action, reward):\n",
    "        # TODO\n",
    "        \n",
    "    def train(self, nb_steps):\n",
    "        # TODO\n",
    "```\n",
    "\n",
    "Re-run the experiment using this Greedy agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "class GreedyAgent:\n",
    "    \n",
    "    def __init__(self, bandit, alpha):\n",
    "        \n",
    "        self.bandit = bandit\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Estimated Q-values\n",
    "        self.Q_t = np.zeros(self.bandit.nb_actions)\n",
    "        \n",
    "    def act(self):\n",
    "        \n",
    "        action = rng.choice(np.where(self.Q_t == self.Q_t.max())[0])\n",
    "        return action\n",
    "        \n",
    "    def update(self, action, reward):\n",
    "        \n",
    "        self.Q_t[action] += self.alpha * (reward - self.Q_t[action])\n",
    "    \n",
    "        \n",
    "    def train(self, nb_steps):\n",
    "        \n",
    "        rewards = []\n",
    "\n",
    "        for step in range(nb_steps):\n",
    "\n",
    "            # Select the action \n",
    "            action = self.act()\n",
    "\n",
    "            # Sample the reward\n",
    "            reward = self.bandit.step(action)\n",
    "\n",
    "            # Store the received reward\n",
    "            rewards.append(reward)\n",
    "\n",
    "            # Update the Q-value estimate of the action\n",
    "            self.update(action, reward)\n",
    "            \n",
    "        return np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "# Learning rate\n",
    "alpha = 0.1\n",
    "\n",
    "# Bandit\n",
    "bandit = Bandit(nb_actions)\n",
    "\n",
    "# Estimates\n",
    "agent = GreedyAgent(bandit, alpha)\n",
    "\n",
    "# Train for 1000 plays\n",
    "rewards = agent.train(1000)\n",
    "    \n",
    "# Plot the Q-values and the evolution of rewards\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(131)\n",
    "plt.bar(range(nb_actions), bandit.Q_star)\n",
    "plt.xlabel(\"Actions\")\n",
    "plt.ylabel(\"$Q^*(a)$\")\n",
    "plt.subplot(132)\n",
    "plt.bar(range(nb_actions), agent.Q_t)\n",
    "plt.xlabel(\"Actions\")\n",
    "plt.ylabel(\"$Q_t(a)$\")\n",
    "plt.subplot(133)\n",
    "plt.plot(rewards)\n",
    "plt.xlabel(\"Plays\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Modify the `train()` method so that it also returns a list of binary values (0 and 1) indicating for each play whether the agent chose the optimal action. Plot this list and observe the lack of exploration.\n",
    "\n",
    "*Hint:* the index of the optimal action is already stored in the bandit: `bandit.a_star`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "class GreedyAgent:\n",
    "    def __init__(self, bandit, alpha):\n",
    "        \n",
    "        self.bandit = bandit\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Estimated Q-values\n",
    "        self.Q_t = np.zeros(self.bandit.nb_actions)\n",
    "        \n",
    "    def act(self):\n",
    "        \n",
    "        action = rng.choice(np.where(self.Q_t == self.Q_t.max())[0])\n",
    "        return action\n",
    "        \n",
    "    def update(self, action, reward):\n",
    "        \n",
    "        self.Q_t[action] += self.alpha * (reward - self.Q_t[action])\n",
    "        \n",
    "    def train(self, nb_steps):\n",
    "        \n",
    "        rewards = []\n",
    "        optimal = []\n",
    "\n",
    "        for step in range(1000):\n",
    "\n",
    "            # Select the action \n",
    "            action = self.act()\n",
    "\n",
    "            # Sample the reward\n",
    "            reward = self.bandit.step(action)\n",
    "\n",
    "            # Store the received reward\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            # Optimal action\n",
    "            if action == self.bandit.a_star:\n",
    "                optimal.append(1.0)\n",
    "            else:\n",
    "                optimal.append(0.0)\n",
    "\n",
    "            # Update the Q-value estimate of the action\n",
    "            self.update(action, reward)\n",
    "            \n",
    "        return np.array(rewards), np.array(optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "# Learning rate\n",
    "alpha = 0.1\n",
    "\n",
    "# Bandit\n",
    "bandit = Bandit(nb_actions)\n",
    "\n",
    "# Estimates\n",
    "agent = GreedyAgent(bandit, alpha)\n",
    "\n",
    "# Store the rewards after each step\n",
    "rewards, optimal = agent.train(1000)\n",
    "    \n",
    "# Plot the Q-values and the evolution of rewards\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(221)\n",
    "plt.bar(range(nb_actions), bandit.Q_star)\n",
    "plt.xlabel(\"Actions\")\n",
    "plt.ylabel(\"$Q^*(a)$\")\n",
    "plt.subplot(222)\n",
    "plt.bar(range(nb_actions), agent.Q_t)\n",
    "plt.xlabel(\"Actions\")\n",
    "plt.ylabel(\"$Q_t(a)$\")\n",
    "plt.subplot(223)\n",
    "plt.plot(rewards)\n",
    "plt.xlabel(\"Plays\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.subplot(224)\n",
    "plt.plot(optimal)\n",
    "plt.xlabel(\"Plays\")\n",
    "plt.ylabel(\"Optimal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evolution of the received rewards and optimal actions does not give a clear indication of the successful learning, as it is strongly dependent on the true Q-values. To truly estimate the performance of the algorithm, we have to average these results over many runs, e.g. 200.\n",
    "\n",
    "**Q:** Run the learning procedure 200 times (new bandit and agent every time) and average the results. Give a unique name to these arrays (e.g. `rewards_greedy` and `optimal_greedy`) as we will do comparisons later. Compare the results with the lecture, where a 10-armed bandit was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "# Number of arms\n",
    "nb_actions = 5\n",
    "\n",
    "# Learning rate\n",
    "alpha = 0.1\n",
    "\n",
    "rewards_greedy = []\n",
    "optimal_greedy = []\n",
    "\n",
    "for trial in range(200):\n",
    "\n",
    "    # Bandit\n",
    "    bandit = Bandit(nb_actions)\n",
    "\n",
    "    # Estimates\n",
    "    agent = GreedyAgent(bandit, alpha)\n",
    "\n",
    "    # Store the rewards after each step\n",
    "    rewards, optimal = agent.train(1000)\n",
    "    \n",
    "    rewards_greedy.append(rewards)\n",
    "    optimal_greedy.append(optimal)\n",
    "    \n",
    "rewards_greedy = np.mean(rewards_greedy, axis=0)\n",
    "optimal_greedy = np.mean(optimal_greedy, axis=0)\n",
    "    \n",
    "# Plot the Q-values and the evolution of rewards\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(121)\n",
    "plt.plot(rewards_greedy)\n",
    "plt.xlabel(\"Plays\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.subplot(122)\n",
    "plt.plot(optimal_greedy)\n",
    "plt.xlabel(\"Plays\")\n",
    "plt.ylabel(\"Optimal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** The greedy agent selects the optimal action around 80% of the time, vs. 50% for the 10-armed bandits. It is really not bad knowing that it starts at chance level (20% for 5 actions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\epsilon$-greedy action selection\n",
    "\n",
    "The main drawback of greedy action selection is that it does not explore: as soon as it finds an action better than the others (with a sufficiently positive true Q-value, i.e. where the sampled rewards are mostly positive), it will keep selecting that action and avoid exploring the other options. \n",
    "\n",
    "The estimated Q-value of the selected action will end up being quite correct, but those of the other actions will stay at 0.\n",
    "\n",
    "In $\\epsilon$-greedy action selection, the greedy action $a_t^*$ (with the highest estimated Q-value) will be selected with a probability $1-\\epsilon$, the others with a probability of $\\epsilon$ altogether. \n",
    "\n",
    "$$\n",
    "    \\pi(a) = \\begin{cases} 1 - \\epsilon \\; \\text{if} \\; a = a_t^* \\\\ \\frac{\\epsilon}{|\\mathcal{A}| - 1} \\; \\text{otherwise.} \\end{cases}\n",
    "$$\n",
    "\n",
    "If you have $|\\mathcal{A}| = 5$ actions, the four non-greedy actions will be selected with a probability of $\\frac{\\epsilon}{4}$.\n",
    "\n",
    "**Q:** Create a `EpsilonGreedyAgent` (possibly inheriting from `GreedyAgent` to reuse code) to implement $\\epsilon$-greedy action selection (with $\\epsilon=0.1$ at first). Do not overwrite the arrays previously calculated (mean reward and optimal actions), as you will want to compare the two methods in a single plot.\n",
    "\n",
    "To implement $\\epsilon-$greedy, you need to:\n",
    "\n",
    "1. Select the greedy action $a = a^*_t$.\n",
    "2. Draw a random number between 0 and 1 (`rng.random()`).\n",
    "3. If this number is smaller than $\\epsilon$, you need to select another action randomly in the remaining ones (`rng.choice()`).\n",
    "4. Otherwise, keep the greedy action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "class EpsilonGreedyAgent(GreedyAgent):\n",
    "    \n",
    "    def __init__(self, bandit, alpha, epsilon):\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # List of actions\n",
    "        self.actions = np.arange(bandit.nb_actions)\n",
    "        \n",
    "        # Call the constructor of GreedyAgent\n",
    "        super().__init__(bandit, alpha)\n",
    "        \n",
    "    def act(self):\n",
    "        \n",
    "        action = rng.choice(np.where(self.Q_t == self.Q_t.max())[0])\n",
    "        \n",
    "        if rng.random() < self.epsilon:\n",
    "            action = rng.choice(self.actions[self.actions != action])\n",
    "            \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "# Number of arms\n",
    "nb_actions = 5\n",
    "\n",
    "# Learning rate\n",
    "alpha = 0.1\n",
    "\n",
    "# Epsilon for exploration\n",
    "epsilon = 0.1\n",
    "\n",
    "rewards_egreedy = []\n",
    "optimal_egreedy = []\n",
    "\n",
    "for trial in range(200):\n",
    "\n",
    "    # Bandit\n",
    "    bandit = Bandit(nb_actions)\n",
    "\n",
    "    # Estimates\n",
    "    agent = EpsilonGreedyAgent(bandit, alpha, epsilon)\n",
    "\n",
    "    # Store the rewards after each step\n",
    "    rewards, optimal = agent.train(1000)\n",
    "    \n",
    "    rewards_egreedy.append(rewards)\n",
    "    optimal_egreedy.append(optimal)\n",
    "    \n",
    "rewards_egreedy = np.mean(rewards_egreedy, axis=0)\n",
    "optimal_egreedy = np.mean(optimal_egreedy, axis=0)\n",
    "    \n",
    "# Plot the Q-values and the evolution of rewards\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(121)\n",
    "plt.plot(rewards_greedy, label=\"Greedy\")\n",
    "plt.plot(rewards_egreedy, label=r\"$\\epsilon$-Greedy\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Plays\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.subplot(122)\n",
    "plt.plot(optimal_greedy, label=\"Greedy\")\n",
    "plt.plot(optimal_egreedy, label=r\"$\\epsilon$-Greedy\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Plays\")\n",
    "plt.ylabel(\"Optimal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Compare the properties of greedy and $\\epsilon$-greedy (speed, optimality, etc). Vary the value of the parameter $\\epsilon$ (0.0001 until 0.5) and conclude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** Depending on the value of $\\epsilon$, $\\epsilon$-greedy can perform better that greedy in the end, but will necessitate more time at the beginning. If there is too much exploration, $\\epsilon$-greedy can be even worse than greedy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax action selection\n",
    "\n",
    "To avoid exploring actions which are clearly not optimal, another useful algorithm is **softmax action selection**. In this scheme, the estimated Q-values are ransformed into a probability distribution using the softmax opertion:\n",
    "\n",
    "$$\n",
    "    \\pi(a) = \\frac{\\exp \\frac{Q_t(a)}{\\tau}}{ \\sum_b \\exp \\frac{Q_t(b)}{\\tau}}\n",
    "$$ \n",
    "\n",
    "For each action, the term $\\exp \\frac{Q_t(a)}{\\tau}$ is proportional to $Q_t(a)$ but made positive. These terms are then normalized by the denominator in order to obtain a sum of 1, i.e. they are the parameters of a discrete probability distribution. The temperature $\\tau$ controls the level of exploration just as $\\epsilon$ for $\\epsilon$-greedy.\n",
    "\n",
    "In practice, $\\exp \\frac{Q_t(a)}{\\tau}$ can be very huge if the Q-values are high or the temperature is small, creating numerical instability (NaN). It is much more stable to substract the maximal Q-value from all Q-values before applying the softmax:\n",
    "\n",
    "$$\n",
    "    \\pi(a) = \\frac{\\exp \\displaystyle\\frac{Q_t(a) - \\max_a Q_t(a)}{\\tau}}{ \\sum_b \\exp \\displaystyle\\frac{Q_t(b) - \\max_b Q_t(b)}{\\tau}}\n",
    "$$ \n",
    "\n",
    "This way, $Q_t(a) - \\max_a Q_t(a)$ is always negative, so its exponential is between 0 and 1.\n",
    "\n",
    "**Q:** Implement the softmax action selection (with $\\tau=0.5$ at first) and compare its performance to greedy and $\\epsilon$-greedy. Vary the temperature $\\tau$ and find the best possible value. Conclude.\n",
    "\n",
    "*Hint:* To select actions with different probabilities, check the doc of `rng.choice()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "class SoftmaxAgent(GreedyAgent):\n",
    "    \n",
    "    def __init__(self, bandit, alpha, tau):\n",
    "        self.tau = tau\n",
    "        \n",
    "        # List of actions\n",
    "        self.actions = np.arange(bandit.nb_actions)\n",
    "        \n",
    "        # Call the constructor of GreedyAgent\n",
    "        super().__init__(bandit, alpha)\n",
    "        \n",
    "    def act(self):\n",
    "        \n",
    "        logit = np.exp((self.Q_t - self.Q_t.max())/self.tau)\n",
    "        \n",
    "        proba_softmax = logit / np.sum(logit)\n",
    "        \n",
    "        action = rng.choice(self.actions, p=proba_softmax) \n",
    "            \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "# Number of arms\n",
    "nb_actions = 5\n",
    "\n",
    "# Learning rate\n",
    "alpha = 0.1\n",
    "\n",
    "# Tau for exploration\n",
    "tau = 0.1\n",
    "\n",
    "rewards_softmax = []\n",
    "optimal_softmax = []\n",
    "\n",
    "for trial in range(200):\n",
    "\n",
    "    # Bandit\n",
    "    bandit = Bandit(nb_actions)\n",
    "\n",
    "    # Estimates\n",
    "    agent = SoftmaxAgent(bandit, alpha, tau)\n",
    "\n",
    "    # Store the rewards after each step\n",
    "    rewards, optimal = agent.train(1000)\n",
    "    \n",
    "    rewards_softmax.append(rewards)\n",
    "    optimal_softmax.append(optimal)\n",
    "    \n",
    "rewards_softmax = np.mean(rewards_softmax, axis=0)\n",
    "optimal_softmax = np.mean(optimal_softmax, axis=0)\n",
    "    \n",
    "# Plot the Q-values and the evolution of rewards\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(121)\n",
    "plt.plot(rewards_greedy, label=\"Greedy\")\n",
    "plt.plot(rewards_egreedy, label=r\"$\\epsilon$-Greedy\")\n",
    "plt.plot(rewards_softmax, label=\"Softmax\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Plays\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.subplot(122)\n",
    "plt.plot(optimal_greedy, label=\"Greedy\")\n",
    "plt.plot(optimal_egreedy, label=r\"$\\epsilon$-Greedy\")\n",
    "plt.plot(optimal_softmax, label=\"Softmax\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Plays\")\n",
    "plt.ylabel(\"Optimal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** Softmax loses less time than $\\epsilon$-greedy exploring the really bad solutions, so it is optimal earlier. It can be more efficient and optimal than the other methods, but finding the right value for $\\tau$ (0.1 works well) is difficult: its optimum value depends on the scaling of Q, you cannot know it in advance..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration scheduling\n",
    "\n",
    "The problem with this version of softmax (with a constant temperature) is that even after it has found the optimal action, it will still explore the other ones (although more rarely than at the beginning). The solution is to **schedule** the exploration parameter so that it explores a lot at the beginning (high temperature) and gradually switches to more exploitation (low temperature).\n",
    "\n",
    "Many schemes are possible for that, the simplest one (**exponential decay**) being to multiply the value of $\\tau$ by a number very close to 1 after **each** play:\n",
    "\n",
    "$$\\tau = \\tau \\times (1 - \\tau_\\text{decay})$$\n",
    "\n",
    "**Q:** Implement in a class `SoftmaxScheduledAgent` temperature scheduling for the softmax algorithm ($\\epsilon$-greedy would be similar) with $\\tau=1$ initially and $\\tau_\\text{decay} = 0.01$ (feel free to change these values). Plot the evolution of `tau` and of the standard deviation of the choices of the optimal action. Conclude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "class SoftmaxScheduledAgent(SoftmaxAgent):\n",
    "    \n",
    "    def __init__(self, bandit, alpha, tau, tau_decay):\n",
    "        self.tau_decay = tau_decay\n",
    "        \n",
    "        self.tau_history = []\n",
    "        \n",
    "        # List of actions\n",
    "        self.actions = np.arange(bandit.nb_actions)\n",
    "        \n",
    "        # Call the constructor of GreedyAgent\n",
    "        super().__init__(bandit, alpha, tau)\n",
    "    \n",
    "        \n",
    "    def act(self):\n",
    "        \n",
    "        # Action selection\n",
    "        logit = np.exp((self.Q_t - self.Q_t.max())/self.tau)\n",
    "        proba_softmax = logit / np.sum(logit)        \n",
    "        action = rng.choice(self.actions, p=proba_softmax) \n",
    "        \n",
    "        # Decay tau\n",
    "        self.tau = self.tau * (1 - self.tau_decay)\n",
    "        self.tau_history.append(self.tau)\n",
    "            \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "# Learning rate\n",
    "alpha = 0.1\n",
    "\n",
    "# Tau for exploration\n",
    "tau = 1.0\n",
    "tau_decay = 0.01\n",
    "\n",
    "rewards_softmaxscheduled = []\n",
    "optimal_softmaxscheduled = []\n",
    "\n",
    "for trial in range(200):\n",
    "\n",
    "    # Bandit\n",
    "    bandit = Bandit(nb_actions)\n",
    "\n",
    "    # Estimates\n",
    "    agent = SoftmaxScheduledAgent(bandit, alpha, tau, tau_decay)\n",
    "\n",
    "    # Store the rewards after each step\n",
    "    rewards, optimal = agent.train(1000)\n",
    "    \n",
    "    rewards_softmaxscheduled.append(rewards)\n",
    "    optimal_softmaxscheduled.append(optimal)\n",
    "    \n",
    "    \n",
    "rewards_softmaxscheduled = np.mean(rewards_softmaxscheduled, axis=0)\n",
    "optimal_softmaxscheduled_std = np.std(optimal_softmaxscheduled, axis=0)\n",
    "optimal_softmaxscheduled = np.mean(optimal_softmaxscheduled, axis=0)\n",
    "    \n",
    "# Plot the Q-values and the evolution of rewards\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(221)\n",
    "plt.plot(rewards_greedy, label=\"Greedy\")\n",
    "plt.plot(rewards_egreedy, label=r\"$\\epsilon$-Greedy\")\n",
    "plt.plot(rewards_softmax, label=\"Softmax\")\n",
    "plt.plot(rewards_softmaxscheduled, label=\"Softmax (scheduled)\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Plays\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.subplot(222)\n",
    "plt.plot(optimal_greedy, label=\"Greedy\")\n",
    "plt.plot(optimal_egreedy, label=r\"$\\epsilon$-Greedy\")\n",
    "plt.plot(optimal_softmax, label=\"Softmax\")\n",
    "plt.plot(optimal_softmaxscheduled, label=\"Softmax (scheduled)\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Plays\")\n",
    "plt.ylabel(\"Optimal\")\n",
    "plt.subplot(223)\n",
    "plt.plot(agent.tau_history)\n",
    "plt.xlabel(\"Plays\")\n",
    "plt.ylabel(\"$\\tau$\")\n",
    "plt.subplot(224)\n",
    "plt.plot(optimal_softmaxscheduled_std)\n",
    "plt.xlabel(\"Plays\")\n",
    "plt.ylabel(\"Variance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** Scheduling drastically improves how often the optimal action is selected. In terms of mean reward, the difference is not that big, as there is often a \"second best\" action whose expected reward is close. We can see that the variance of the optimal action selection follows the parameter $\\tau$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Experiment with different schedules (initial values, decay rate) and try to find the best setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "# Learning rate\n",
    "alpha = 0.1\n",
    "\n",
    "# Tau for exploration\n",
    "tau = 10.0\n",
    "tau_decay = 0.05\n",
    "\n",
    "rewards_softmaxscheduled = []\n",
    "optimal_softmaxscheduled = []\n",
    "\n",
    "for trial in range(200):\n",
    "\n",
    "    # Bandit\n",
    "    bandit = Bandit(nb_actions)\n",
    "\n",
    "    # Estimates\n",
    "    agent = SoftmaxScheduledAgent(bandit, alpha, tau, tau_decay)\n",
    "\n",
    "    # Store the rewards after each step\n",
    "    rewards, optimal = agent.train(1000)\n",
    "    \n",
    "    rewards_softmaxscheduled.append(rewards)\n",
    "    optimal_softmaxscheduled.append(optimal)\n",
    "    \n",
    "    \n",
    "rewards_softmaxscheduled = np.mean(rewards_softmaxscheduled, axis=0)\n",
    "optimal_softmaxscheduled_std = np.std(optimal_softmaxscheduled, axis=0)\n",
    "optimal_softmaxscheduled = np.mean(optimal_softmaxscheduled, axis=0)\n",
    "    \n",
    "# Plot the Q-values and the evolution of rewards\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(221)\n",
    "plt.plot(rewards_greedy, label=\"Greedy\")\n",
    "plt.plot(rewards_egreedy, label=r\"$\\epsilon$-Greedy\")\n",
    "plt.plot(rewards_softmax, label=\"Softmax\")\n",
    "plt.plot(rewards_softmaxscheduled, label=\"Softmax (scheduled)\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Plays\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.subplot(222)\n",
    "plt.plot(optimal_greedy, label=\"Greedy\")\n",
    "plt.plot(optimal_egreedy, label=r\"$\\epsilon$-Greedy\")\n",
    "plt.plot(optimal_softmax, label=\"Softmax\")\n",
    "plt.plot(optimal_softmaxscheduled, label=\"Softmax (scheduled)\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Plays\")\n",
    "plt.ylabel(\"Optimal\")\n",
    "plt.subplot(223)\n",
    "plt.plot(agent.tau_history)\n",
    "plt.xlabel(\"Plays\")\n",
    "plt.ylabel(\"$\\tau$\")\n",
    "plt.subplot(224)\n",
    "plt.plot(optimal_softmaxscheduled_std)\n",
    "plt.xlabel(\"Plays\")\n",
    "plt.ylabel(\"Variance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** No unique answer here, but a very high exploration parameter initially which decreases quite fast leads to very performant solutions. Take-home message: scheduling is very important, but it is quite difficult to find the optimal schedule."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
