{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN\n",
    "\n",
    "The goal of this exercise is to implement DQN using pytorch and to apply it to the cartpole balancing problem. \n",
    "\n",
    "The code is adapted from the Pytorch tutorial: <https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install -U gymnasium pygame moviepy swig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gym version: 1.0.0\n",
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Default libraries\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "rng = np.random.default_rng()\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "# Gymnasium\n",
    "import gymnasium as gym\n",
    "print(\"gym version:\", gym.__version__)\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Select hardware: \n",
    "if torch.cuda.is_available(): # GPU\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available(): # Metal (Macos)\n",
    "    device = torch.device(\"mps\")\n",
    "else: # CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import ImageSequenceClip, ipython_display\n",
    "\n",
    "class GymRecorder(object):\n",
    "    \"\"\"\n",
    "    Simple wrapper over moviepy to generate a .gif with the frames of a gym environment.\n",
    "    \n",
    "    The environment must have the render_mode `rgb_array_list`.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self._frames = []\n",
    "\n",
    "    def record(self, frames):\n",
    "        \"To be called at the end of an episode.\"\n",
    "        for frame in frames:\n",
    "            self._frames.append(np.array(frame))\n",
    "\n",
    "    def make_video(self, filename):\n",
    "        \"Generates the gif video.\"\n",
    "        directory = os.path.dirname(os.path.abspath(filename))\n",
    "        if not os.path.exists(directory):\n",
    "            os.mkdir(directory)\n",
    "        self.clip = ImageSequenceClip(list(self._frames), fps=self.env.metadata[\"render_fps\"])\n",
    "        self.clip.write_gif(filename, fps=self.env.metadata[\"render_fps\"], loop=0)\n",
    "        del self._frames\n",
    "        self._frames = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cartpole balancing task\n",
    "\n",
    "We are going to use the Cartpole balancing problem, which can be loaded with:\n",
    "\n",
    "```python\n",
    "gym.make('CartPole-v0', render_mode=\"rgb_array_list\")\n",
    "```\n",
    "\n",
    "States have 4 continuous values (position and speed of the cart, angle and speed of the pole) and 2 discrete outputs (going left or right). The reward is +1 for each transition where the pole is still standing (angle of less than 30Â° with the vertical). \n",
    "\n",
    "In CartPole-v0, the episode ends when the pole fails or after 200 steps. In CartPole-v1, the maximum episode length is 500 steps, which is too long for us, so we stick to v0 here.\n",
    "\n",
    "The maximal (undiscounted) return is therefore 200. Can DQN learn this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return: 17.0\n",
      "MoviePy - Building file videos/cartpole_random.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div align=middle><img  src='data:image/gif;base64,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'></div>"
      ],
      "text/plain": [
       "<moviepy.video.io.html_tools.HTML2 object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = gym.make('CartPole-v0', render_mode=\"rgb_array_list\")\n",
    "recorder = GymRecorder(env)\n",
    "\n",
    "# Sample the initial state\n",
    "state, info = env.reset()\n",
    "\n",
    "# One episode:\n",
    "done = False\n",
    "return_episode = 0\n",
    "while not done:\n",
    "\n",
    "    # Select an action randomly\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Sample a single transition\n",
    "    next_state, reward, terminal, truncated, info = env.step(action)\n",
    "\n",
    "    # End of the episode\n",
    "    done = terminal or truncated\n",
    "\n",
    "    # Update undiscounted return\n",
    "    return_episode += reward\n",
    "    \n",
    "    # Go in the next state\n",
    "    state = next_state\n",
    "\n",
    "print(\"Return:\", return_episode)\n",
    "\n",
    "recorder.record(env.render())\n",
    "video = \"videos/cartpole_random.gif\"\n",
    "recorder.make_video(video)\n",
    "ipython_display(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value network in pytorch\n",
    "\n",
    "As the state in Cartpole has only four dimensions, we do not need a CNN for the value network. A simple MLP with a couple of hidden layers will be enough.\n",
    "\n",
    "**Q:** Create a MLP class in pytorch taking four inputs and two outputs (one Q-value per action), and two hidden layers of 128 neurons (you can change it later). If possible, make it parameterizable, i.e. have the constructor take in the number of inputs, outputs and hidden neurons. The activation function for the hidden layers should be ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    \"Value network for DQN on Cartpole.\"\n",
    "\n",
    "    def __init__(self, nb_observations, nb_hidden1, nb_hidden2, nb_actions):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # Layers\n",
    "        self.fc1 = torch.nn.Linear(nb_observations, nb_hidden1)\n",
    "        self.fc2 = torch.nn.Linear(nb_hidden1, nb_hidden2)\n",
    "        self.fc3 = torch.nn.Linear(nb_hidden2, nb_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Create a network, an environment, get the initial state using `env.reset()` and pass it to the `forward()` method of your NN. What happens?\n",
    "\n",
    "Do not forget to send the network to your device, especially if you have a GPU. Create the network using something like:\n",
    "\n",
    "```python\n",
    "net = MLP(...).to(device)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0224134   0.00670727 -0.03332475 -0.03872823]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(state)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Predict the Q-values from the initial state\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m Q_values \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n",
      "File \u001b[0;32m~/Teaching/DeepReinforcementLearning/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Teaching/DeepReinforcementLearning/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Teaching/DeepReinforcementLearning/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Create the value network\n",
    "net = MLP(env.observation_space.shape[0], 128, 128, env.action_space.n).to(device)\n",
    "\n",
    "# Sample the initial state\n",
    "state, info = env.reset()\n",
    "print(state)\n",
    "\n",
    "# Predict the Q-values from the initial state\n",
    "Q_values = net.forward(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we need to cast the state vector into a pytorch tensor, pytorch does not do it automatically.\n",
    "\n",
    "To cast a numpy vector of shape (4,) into a tensor, one simply needs to call:\n",
    "\n",
    "```python\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "```\n",
    "\n",
    "The dtype must be set to `torch.float32` for floating numbers. Integers should be set to `torch.long`. Do not forget to send the tensor to your device if you plan to pass it to your network.\n",
    "\n",
    "**Q:** Pass the new tensor to your network. What is the shape of the output tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03727312 0.02480382 0.03185986 0.00359333] (4,)\n",
      "tensor([0.0373, 0.0248, 0.0319, 0.0036], device='mps:0') torch.Size([4])\n",
      "tensor([0.0286, 0.0517], device='mps:0', grad_fn=<LinearBackward0>) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Create the value network\n",
    "net = MLP(env.observation_space.shape[0], 128, 128, env.action_space.n).to(device)\n",
    "\n",
    "# Sample the initial state\n",
    "state, info = env.reset()\n",
    "print(state, state.shape)\n",
    "\n",
    "# Cast the state to a tensor\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "print(state, state.shape)\n",
    "\n",
    "# Predict the Q-values from the initial state\n",
    "Q_values = net.forward(state)\n",
    "print(Q_values, Q_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value network outputs one Q-value per action, great. Now, let's identify the **greedy** action, i.e. the one with the highest Q-value. The two actions expected by the cartpole environment are 0 and 1, i.e. the index of the element with the highest Q-value as a Python integer. \n",
    "\n",
    "Have a look at those two methods of `Tensor`:\n",
    "\n",
    "* ``Tensor.argmax``: <https://pytorch.org/docs/stable/generated/torch.argmax.html>\n",
    "* ``Tensor.item``: <https://pytorch.org/docs/stable/generated/torch.Tensor.item.html>\n",
    "\n",
    "**Q:** Find a way to obtain the index (as a Python integer) of the element with the highest value in the tensor of Q-values. Check that it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy action: 1\n"
     ]
    }
   ],
   "source": [
    "greedy_action = Q_values.argmax().item()\n",
    "print(f\"Greedy action: {greedy_action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Create a dummy agent class (as in the previous exercises) storing a value network and acting using $\\epsilon$-greedy action selection. Add a ``test()``method running a few episodes and possibly recording them. \n",
    "\n",
    "The constructor should accept several hyperparameters, such as the `config` dictionary in the following skeleton:\n",
    "\n",
    "```python\n",
    "class RandomDQNAgent:\n",
    "    def __init__(self, env, config):\n",
    "    def act(self, state):\n",
    "    def test(self, nb_episodes, recorder=None):\n",
    "```\n",
    "\n",
    "To prepare ourselves, implement a schedule for `epsilon` in the `act()` method: epsilon should start at a high value of 0.9 and decrease exponentially to 0.05 for each action made. The value of epsilon follows this formula:\n",
    "\n",
    "$$\n",
    "    \\epsilon = 0.05 + (0.9 - 0.05) * \\exp ( - \\dfrac{t}{1000})\n",
    "$$\n",
    "\n",
    "where t is the number of steps since the start. 0.05, 0.9 and 1000 should be parameters of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDQNAgent:\n",
    "    \"\"\"\n",
    "    Random deep Q-learning agent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, config):\n",
    "\n",
    "        self.env = env\n",
    "        self.config = config\n",
    "        self.epsilon = self.config['eps_start']\n",
    "\n",
    "        # Number of actions\n",
    "        self.n_actions = self.env.action_space.n\n",
    "\n",
    "        # Number of states\n",
    "        self.state, info = self.env.reset()\n",
    "        self.n_observations = len(self.state)\n",
    "\n",
    "        # Value network\n",
    "        self.value_net = MLP(self.n_observations, config['nb_hidden'], config['nb_hidden'], self.n_actions).to(device)\n",
    "\n",
    "        self.steps_done = 0\n",
    "        \n",
    "    \n",
    "    def act(self, state):\n",
    "        \"Returns an action using epsilon-greedy action selection.\"\n",
    "\n",
    "        # Decay epsilon exponentially\n",
    "        self.epsilon = self.config['eps_end'] + (self.config['eps_start'] - self.config['eps_end']) * math.exp(-1. * self.steps_done / self.config['eps_decay'])\n",
    "\n",
    "        # Keep track of time\n",
    "        self.steps_done += 1\n",
    "    \n",
    "        # epsilon-greedy action selection\n",
    "        if rng.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                return self.value_net(state).argmax().item()\n",
    "\n",
    "    \n",
    "    def test(self, nb_episodes, recorder=None):\n",
    "        \"Performs a test episode without exploration.\"\n",
    "        previous_epsilon = self.epsilon\n",
    "        self.epsilon = 0.0\n",
    "\n",
    "        for episode in range(nb_episodes):\n",
    "        \n",
    "            # Reset\n",
    "            state, _ = self.env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "\n",
    "            # Sample the episode\n",
    "            done = False\n",
    "            return_episode = 0\n",
    "            while not done:                \n",
    "                action = self.act(state)\n",
    "                next_state, reward, terminal, truncated, info = self.env.step(action)\n",
    "                return_episode += reward\n",
    "                done = terminal or truncated\n",
    "                state = torch.tensor(next_state, dtype=torch.float32, device=device)\n",
    "\n",
    "            print(f\"Episode {episode}: return {return_episode}, epsilon: {self.epsilon:.4f}\")\n",
    "            \n",
    "        self.epsilon = previous_epsilon\n",
    "            \n",
    "        if recorder is not None:\n",
    "            recorder.record(self.env.render())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: return 23.0, epsilon: 0.8815\n",
      "Episode 1: return 12.0, epsilon: 0.8716\n",
      "Episode 2: return 13.0, epsilon: 0.8610\n",
      "Episode 3: return 17.0, epsilon: 0.8473\n",
      "Episode 4: return 23.0, epsilon: 0.8292\n",
      "Episode 5: return 37.0, epsilon: 0.8009\n",
      "Episode 6: return 30.0, epsilon: 0.7787\n",
      "Episode 7: return 23.0, epsilon: 0.7621\n",
      "Episode 8: return 16.0, epsilon: 0.7508\n",
      "Episode 9: return 31.0, epsilon: 0.7294\n",
      "MoviePy - Building file videos/cartpole-random2.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div align=middle><img  src='data:image/gif;base64,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'></div>"
      ],
      "text/plain": [
       "<moviepy.video.io.html_tools.HTML2 object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = gym.make('CartPole-v0', render_mode=\"rgb_array_list\")\n",
    "recorder = GymRecorder(env)\n",
    "\n",
    "# Hyperparameters\n",
    "config = {}\n",
    "config['nb_hidden'] = 128 # number of hidden neurons in each layer\n",
    "config['eps_start'] = 0.9 # starting value of epsilon\n",
    "config['eps_end'] = 0.05 # final value of epsilon\n",
    "config['eps_decay'] = 1000 # rate of exponential decay of epsilon, higher means a slower decay\n",
    "\n",
    "# Create the agent\n",
    "agent = RandomDQNAgent(env, config)\n",
    "\n",
    "# Make 10 evaluation episodes\n",
    "agent.test(10, recorder)\n",
    "\n",
    "# Show episodes\n",
    "recorder.record(env.render())\n",
    "video = \"videos/cartpole-random2.gif\"\n",
    "recorder.make_video(video)\n",
    "ipython_display(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target network\n",
    "\n",
    "The original DQN algorithm implies two neural networks:\n",
    "\n",
    "1. The value network $Q_\\theta(s, a)$, learning to predict the Q-values for the current state.\n",
    "2. The target network $Q_{\\theta'}(s, a)$, used to predict the Q-values in the next state.\n",
    "\n",
    "The target network is a copy of the value network (in terms of structure and parameters), but the update occurs only from time to time.\n",
    "\n",
    "**Q:** Create two MLPs of the same size and predict the Q-values of a single state. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0006,  0.0357], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "tensor([ 0.0151, -0.0160], device='mps:0', grad_fn=<LinearBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Create the value network\n",
    "value_net = MLP(env.observation_space.shape[0], 128, 128, env.action_space.n).to(device)\n",
    "\n",
    "# Create the target network\n",
    "target_net = MLP(env.observation_space.shape[0], 128, 128, env.action_space.n).to(device)\n",
    "\n",
    "# Sample the initial state\n",
    "state, _ = env.reset()\n",
    "\n",
    "# Cast the state to a tensor\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "\n",
    "# Predict the Q-values for both networks\n",
    "Q_value = value_net.forward(state)\n",
    "Q_target = target_net.forward(state)\n",
    "\n",
    "print(Q_value)\n",
    "print(Q_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, the two MLPs are initialized using random parameters, so they are different. We need a method to copy the weights of a network into another one. \n",
    "\n",
    "It is fortunately very easy to save/load the parameters of a pytorch network:\n",
    "\n",
    "```python\n",
    "params = net.state_dict()\n",
    "net.load_state_dict(params)\n",
    "```\n",
    "\n",
    "**Q:** Apply these methods to update the weights of the target network with the value one. Check that they now predict the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0105, -0.1769], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "tensor([ 0.0105, -0.1769], device='mps:0', grad_fn=<LinearBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Create the value network\n",
    "value_net = MLP(env.observation_space.shape[0], 128, 128, env.action_space.n).to(device)\n",
    "\n",
    "# Create the target network\n",
    "target_net = MLP(env.observation_space.shape[0], 128, 128, env.action_space.n).to(device)\n",
    "\n",
    "# Update the target network\n",
    "target_net.load_state_dict(value_net.state_dict())\n",
    "\n",
    "# Sample the initial state\n",
    "state, _ = env.reset()\n",
    "\n",
    "# Cast the state to a tensor\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "\n",
    "# Predict the Q-values for both networks\n",
    "Q_value = value_net.forward(state)\n",
    "Q_target = target_net.forward(state)\n",
    "\n",
    "print(Q_value)\n",
    "print(Q_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay Memory\n",
    "\n",
    "The second important component of DQN is the experience replay memory (ERM) or replay buffer. It is a limited size buffer that can store $(s, a, r, s', d)$ transitions, where $d$ is a boolean indicating whether the next state $s'$ is terminal or not (in gymnasium, this is the boolean `done = terminal or truncated`).\n",
    "\n",
    "Below is a simple implementation of an ERM. The important data structure here is `deque` (double-ended queue) which behaves like a list when `append()` is called, until its capacity is reached (`maxlen`), in which case new elements overwrite older ones. \n",
    "\n",
    "`batch = sample(batch_size)` randomly samples a minibatch from the ERM and returns a structure of $(s, a, r, s', d)$ transitions, nicely casted into pytorch tensors. These tensors are accessed with `batch.state`, `batch.action`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named tuples are fancy dictionaries\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \"Simple Experience Replay Memory using uniform sampling.\"\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        \"Appends a transition (s, a, r, s', done) to the buffer.\"\n",
    "        # Get numpy arrays\n",
    "        if isinstance(state, (torch.Tensor,)): state = state.numpy(force=True)\n",
    "        if isinstance(next_state, (torch.Tensor,)): next_state = next_state.numpy(force=True)\n",
    "        \n",
    "        # Append to the buffer\n",
    "        self.memory.append(Transition(state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"Returns a minibatch of (s, a, r, s', done)\"\n",
    "        # Sample the batch\n",
    "        transitions = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        # Transpose the batch.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        # Cast to tensors\n",
    "        states = torch.tensor(batch.state, dtype=torch.float32, device=device)\n",
    "        actions = torch.tensor(batch.action, dtype=torch.long, device=device)\n",
    "        rewards = torch.tensor(batch.reward, dtype=torch.float32, device=device)\n",
    "        next_states = torch.tensor(batch.next_state, dtype=torch.float32, device=device)\n",
    "        dones = torch.tensor(batch.done, dtype=torch.bool, device=device)\n",
    "\n",
    "        return Transition(states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Modify your random DQN agent so that it stores a replay buffer of capacity 10000 and appends all transitions into it. Do a few episodes, sample a small minibatch and have a look at the data you obtain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDQNAgent:\n",
    "    \"\"\"\n",
    "    Random deep Q-learning agent with memory.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, config):\n",
    "\n",
    "        self.env = env\n",
    "        self.config = config\n",
    "        self.epsilon = self.config['eps_start']\n",
    "\n",
    "        # Number of actions\n",
    "        self.n_actions = self.env.action_space.n\n",
    "\n",
    "        # Number of states\n",
    "        self.state, info = self.env.reset()\n",
    "        self.n_observations = len(self.state)\n",
    "\n",
    "        # Value network\n",
    "        self.value_net = MLP(self.n_observations, config['nb_hidden'], config['nb_hidden'], self.n_actions).to(device)\n",
    "\n",
    "        # Replay buffer\n",
    "        self.memory = ReplayMemory(capacity=1000)\n",
    "\n",
    "        self.steps_done = 0\n",
    "        \n",
    "    \n",
    "    def act(self, state):\n",
    "        \"Returns an action using epsilon-greedy action selection.\"\n",
    "\n",
    "        # Decay epsilon exponentially\n",
    "        self.epsilon = self.config['eps_end'] + (self.config['eps_start'] - self.config['eps_end']) * math.exp(-1. * self.steps_done / self.config['eps_decay'])\n",
    "\n",
    "        # Keep track of time\n",
    "        self.steps_done += 1\n",
    "    \n",
    "        # epsilon-greedy action selection\n",
    "        if rng.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                return self.value_net(state).argmax().item()\n",
    "\n",
    "    \n",
    "    def test(self, nb_episodes, recorder=None):\n",
    "        \"Performs a test episode without exploration.\"\n",
    "        previous_epsilon = self.epsilon\n",
    "        self.epsilon = 0.0\n",
    "\n",
    "        for episode in range(nb_episodes):\n",
    "        \n",
    "            # Reset\n",
    "            state, _ = self.env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "\n",
    "            # Sample the episode\n",
    "            done = False\n",
    "            return_episode = 0\n",
    "            while not done:                \n",
    "                action = self.act(state)\n",
    "                next_state, reward, terminal, truncated, info = self.env.step(action)\n",
    "                return_episode += reward\n",
    "                done = terminal or truncated\n",
    "                \n",
    "                # Append the transition to the replay buffer\n",
    "                self.memory.append(state, action, reward, next_state, done)\n",
    "\n",
    "                state = torch.tensor(next_state, dtype=torch.float32, device=device)\n",
    "\n",
    "            print(f\"Episode {episode}: return {return_episode}, epsilon: {self.epsilon:.4f}\")\n",
    "            \n",
    "        self.epsilon = previous_epsilon\n",
    "            \n",
    "        if recorder is not None:\n",
    "            recorder.record(self.env.render())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: return 19.0, epsilon: 0.8848\n",
      "Episode 1: return 8.0, epsilon: 0.8782\n",
      "Episode 2: return 51.0, epsilon: 0.8370\n",
      "Episode 3: return 40.0, epsilon: 0.8061\n",
      "Episode 4: return 32.0, epsilon: 0.7823\n",
      "Episode 5: return 12.0, epsilon: 0.7736\n",
      "Episode 6: return 14.0, epsilon: 0.7635\n",
      "Episode 7: return 35.0, epsilon: 0.7390\n",
      "Episode 8: return 19.0, epsilon: 0.7260\n",
      "Episode 9: return 25.0, epsilon: 0.7093\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Hyperparameters\n",
    "config = {}\n",
    "config['nb_hidden'] = 128 # number of hidden neurons in each layer\n",
    "config['eps_start'] = 0.9 # starting value of epsilon\n",
    "config['eps_end'] = 0.05 # final value of epsilon\n",
    "config['eps_decay'] = 1000 # rate of exponential decay of epsilon, higher means a slower decay\n",
    "config['buffer_limit'] = 1000 # maximum number of transitions in the replay buffer\n",
    "\n",
    "# Create the agent\n",
    "agent = RandomDQNAgent(env, config)\n",
    "\n",
    "# Make 10 evaluation episodes\n",
    "agent.test(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: torch.Size([10, 4]) tensor([[-8.8130e-03, -1.8004e-01, -1.8154e-02,  2.9670e-01],\n",
      "        [ 8.2345e-03, -4.3172e-02, -3.3840e-02, -1.3027e-01],\n",
      "        [-1.5566e-01, -8.1054e-01,  1.9792e-01,  1.4256e+00],\n",
      "        [-2.2250e-02,  1.5405e-02,  4.4516e-03, -3.2377e-03],\n",
      "        [ 4.8573e-02,  8.2803e-03, -1.0452e-03,  9.8198e-03],\n",
      "        [-4.1510e-02, -2.1378e-01,  2.0494e-02,  2.7213e-01],\n",
      "        [ 2.2593e-02, -4.1378e-01, -2.5314e-02,  6.1285e-01],\n",
      "        [-1.3218e-02,  1.5322e-02, -7.3846e-03, -1.4009e-03],\n",
      "        [-1.1026e-01, -7.7357e-01,  1.5540e-01,  1.3686e+00],\n",
      "        [-4.8141e-02, -2.4915e-01,  7.8527e-02,  4.3004e-01]], device='mps:0')\n",
      "Actions: torch.Size([10]) tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0], device='mps:0')\n",
      "Rewards: torch.Size([10]) tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='mps:0')\n",
      "Next states: torch.Size([10, 4]) tensor([[-1.2414e-02,  1.5334e-02, -1.2220e-02, -1.6545e-03],\n",
      "        [ 7.3711e-03, -2.3779e-01, -3.6446e-02,  1.5155e-01],\n",
      "        [-1.7187e-01, -6.1834e-01,  2.2643e-01,  1.2007e+00],\n",
      "        [-2.1942e-02,  2.1046e-01,  4.3868e-03, -2.9451e-01],\n",
      "        [ 4.8739e-02, -1.8683e-01, -8.4878e-04,  3.0217e-01],\n",
      "        [-4.5785e-02, -4.0919e-01,  2.5937e-02,  5.7120e-01],\n",
      "        [ 1.4318e-02, -6.0854e-01, -1.3057e-02,  8.9745e-01],\n",
      "        [-1.2912e-02, -1.7969e-01, -7.4126e-03,  2.8894e-01],\n",
      "        [-1.2573e-01, -9.7026e-01,  1.8277e-01,  1.7056e+00],\n",
      "        [-5.3124e-02, -4.4529e-01,  8.7128e-02,  7.4641e-01]], device='mps:0')\n",
      "Dones: torch.Size([10]) tensor([False, False,  True, False, False, False, False, False, False, False],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# Sample the ERM\n",
    "batch = agent.memory.sample(10)\n",
    "\n",
    "print(\"States:\", batch.state.shape, batch.state)\n",
    "print(\"Actions:\", batch.action.shape, batch.action)\n",
    "print(\"Rewards:\", batch.reward.shape, batch.reward)\n",
    "print(\"Next states:\", batch.next_state.shape, batch.next_state)\n",
    "print(\"Dones:\", batch.done.shape, batch.done)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Use the value network stored into your agent to predict the Q-values of all actions for the states contained in the minibatch. Do NOT use a for loop. Check the size of the resulting tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 2])\n",
      "tensor([[0.1208, 0.0245],\n",
      "        [0.0820, 0.0050],\n",
      "        [0.1191, 0.1216],\n",
      "        [0.0879, 0.0212],\n",
      "        [0.0916, 0.0177],\n",
      "        [0.1140, 0.0217],\n",
      "        [0.1230, 0.0468],\n",
      "        [0.0891, 0.0205],\n",
      "        [0.1215, 0.1166],\n",
      "        [0.1190, 0.0369]], device='mps:0', grad_fn=<LinearBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Q_values = agent.value_net.forward(batch.state)\n",
    "\n",
    "print(Q_values.shape)\n",
    "print(Q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** The previous tensors returns the value of all actions in those visited states. We now want only the Q-value of action that was taken (whose index is in `batch.action`). The resulting tensor should therefore a vector of length `batch_size`. How do we do that?\n",
    "\n",
    "*Hint:* it would take months of practice to master all the indexing methods available in pytorch: <https://pytorch.org/docs/stable/torch.html#indexing-slicing-joining>. Meanwhile, numpy-style indexing could be useful. Check what the following statements do:\n",
    "\n",
    "```python\n",
    "N = 10\n",
    "A = torch.randn((N, 2))\n",
    "B = torch.randint(0, 2, (N,))\n",
    "C = A[range(N), B]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1208, 0.0245],\n",
      "        [0.0820, 0.0050],\n",
      "        [0.1191, 0.1216],\n",
      "        [0.0879, 0.0212],\n",
      "        [0.0916, 0.0177],\n",
      "        [0.1140, 0.0217],\n",
      "        [0.1230, 0.0468],\n",
      "        [0.0891, 0.0205],\n",
      "        [0.1215, 0.1166],\n",
      "        [0.1190, 0.0369]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0], device='mps:0')\n",
      "tensor([0.0245, 0.0820, 0.1216, 0.0212, 0.0916, 0.1140, 0.1230, 0.0891, 0.1215,\n",
      "        0.1190], device='mps:0', grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(Q_values)\n",
    "print(batch.action)\n",
    "Q_taken = Q_values[range(10), batch.action]\n",
    "print(Q_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"DQN agent.\"\n",
    "    \n",
    "    def __init__(self, env, config):\n",
    "\n",
    "        # Parameters\n",
    "        self.env = env\n",
    "        self.config = config\n",
    "\n",
    "        # Number of actions\n",
    "        self.n_actions = self.env.action_space.n\n",
    "\n",
    "        # Number of states\n",
    "        self.state, info = self.env.reset()\n",
    "        self.n_observations = len(self.state)\n",
    "\n",
    "        # Value network\n",
    "        self.value_net = MLP(self.n_observations, config['nb_hidden'], config['nb_hidden'], self.n_actions).to(device)\n",
    "\n",
    "        # Target network\n",
    "        self.target_net = MLP(self.n_observations, config['nb_hidden'], config['nb_hidden'], self.n_actions).to(device)\n",
    "\n",
    "        # Copy the value weights into the target network\n",
    "        self.target_net.load_state_dict(self.value_net.state_dict())\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.value_net.parameters(), lr=self.config['learning_rate'])\n",
    "\n",
    "        # Loss function\n",
    "        self.loss_function = torch.nn.MSELoss()\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.memory = ReplayMemory(self.config['buffer_limit'])\n",
    "\n",
    "        self.steps_done = 0\n",
    "        self.episode_durations = []\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "\n",
    "        # Decay epsilon exponentially\n",
    "        self.epsilon = self.config['eps_end'] + (self.config['eps_start'] - self.config['eps_end']) * math.exp(-1. * self.steps_done / self.config['eps_decay'])\n",
    "\n",
    "        # Keep track of time\n",
    "        self.steps_done += 1\n",
    "    \n",
    "        # epsilon-greedy action selection\n",
    "        if rng.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                return self.value_net(state).argmax(dim=0).item()\n",
    "\n",
    "    def update(self):\n",
    "\n",
    "        # Only learn when the replay buffer is full enough\n",
    "        if len(self.memory) < 2 * self.config['batch_size']:\n",
    "            return\n",
    "        \n",
    "        # Sample a batch\n",
    "        batch = self.memory.sample(self.config['batch_size'])\n",
    "\n",
    "        # Compute Q(s_t, a) with the current value network.\n",
    "        #Q_values = self.value_net(batch.state).gather(1, batch.action.unsqueeze(1)).squeeze(1)\n",
    "        Q_values = self.value_net(batch.state)[range(self.config['batch_size']), batch.action]\n",
    "        \n",
    "        # Compute Q(s_{t+1}, a*) for all next states.\n",
    "        # If the next state is terminal, set the value to zero.\n",
    "        # Do not compute gradients.\n",
    "        with torch.no_grad():\n",
    "            next_Q_values = self.target_net(batch.next_state).max(dim=1).values\n",
    "            next_Q_values[batch.done] = 0.0\n",
    "\n",
    "        # Compute the target Q values\n",
    "        targets = (next_Q_values * self.config['gamma']) + batch.reward\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.loss_function(Q_values, targets)\n",
    "\n",
    "        # Reinitialize the gradients\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # In-place gradient clipping\n",
    "        torch.nn.utils.clip_grad_value_(self.value_net.parameters(), 100)\n",
    "\n",
    "        # Optimizer step\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        \n",
    "        for i_episode in range(num_episodes):\n",
    "\n",
    "            tstart = time.time()\n",
    "\n",
    "            # Initialize the environment and get its state\n",
    "            state, _ = self.env.reset()\n",
    "\n",
    "            # Transform the state into a tensor\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "\n",
    "            done = False\n",
    "            steps_episode = 0\n",
    "            while not done:\n",
    "                \n",
    "                # Select an action\n",
    "                action = self.act(state)\n",
    "                \n",
    "                # Perform the action\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                \n",
    "                # Terminal state\n",
    "                done = terminated or truncated\n",
    "\n",
    "                # Store the transition in memory\n",
    "                self.memory.append(state, action, reward, next_state, done)\n",
    "\n",
    "                # Move to the next state\n",
    "                state = torch.tensor(next_state, dtype=torch.float32, device=device)\n",
    "\n",
    "                # Perform one step of the optimization (on the policy network)\n",
    "                self.update()\n",
    "\n",
    "                # Update of the target network's weights\n",
    "                if self.steps_done % self.config['target_update_period'] == 0:\n",
    "                    self.target_net.load_state_dict(self.value_net.state_dict())\n",
    "\n",
    "                # Finish episode\n",
    "                steps_episode += 1\n",
    "                if done:\n",
    "                    self.episode_durations.append(steps_episode)\n",
    "                    print(f\"Episode {i_episode+1}, duration {steps_episode}, epsilon {self.epsilon:.4f} done in {time.time() - tstart}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, duration 29, epsilon 0.8765 done in 0.018595218658447266\n",
      "Episode 2, duration 14, epsilon 0.8650 done in 0.010457038879394531\n",
      "Episode 3, duration 27, epsilon 0.8433 done in 0.015136957168579102\n",
      "Episode 4, duration 30, epsilon 0.8199 done in 0.01650214195251465\n",
      "Episode 5, duration 32, epsilon 0.7956 done in 0.01904582977294922\n",
      "Episode 6, duration 29, epsilon 0.7743 done in 0.018700122833251953\n",
      "Episode 7, duration 13, epsilon 0.7650 done in 0.00727081298828125\n",
      "Episode 8, duration 32, epsilon 0.7425 done in 0.019001007080078125\n",
      "Episode 9, duration 30, epsilon 0.7220 done in 0.01921367645263672\n",
      "Episode 10, duration 38, epsilon 0.6969 done in 0.14808130264282227\n",
      "Episode 11, duration 16, epsilon 0.6867 done in 0.11038994789123535\n",
      "Episode 12, duration 13, epsilon 0.6784 done in 0.08904290199279785\n",
      "Episode 13, duration 12, epsilon 0.6709 done in 0.08433914184570312\n",
      "Episode 14, duration 14, epsilon 0.6623 done in 0.09883522987365723\n",
      "Episode 15, duration 12, epsilon 0.6550 done in 0.0849311351776123\n",
      "Episode 16, duration 15, epsilon 0.6460 done in 0.10664010047912598\n",
      "Episode 17, duration 13, epsilon 0.6383 done in 0.09975004196166992\n",
      "Episode 18, duration 12, epsilon 0.6313 done in 0.09827208518981934\n",
      "Episode 19, duration 42, epsilon 0.6074 done in 0.33676600456237793\n",
      "Episode 20, duration 54, epsilon 0.5781 done in 0.43784189224243164\n",
      "Episode 21, duration 46, epsilon 0.5543 done in 0.36170101165771484\n",
      "Episode 22, duration 31, epsilon 0.5389 done in 0.25875091552734375\n",
      "Episode 23, duration 28, epsilon 0.5254 done in 0.21550679206848145\n",
      "Episode 24, duration 21, epsilon 0.5156 done in 0.16527986526489258\n",
      "Episode 25, duration 13, epsilon 0.5095 done in 0.09794878959655762\n",
      "Episode 26, duration 200, epsilon 0.4262 done in 1.4106419086456299\n",
      "Episode 27, duration 42, epsilon 0.4108 done in 0.32305097579956055\n",
      "Episode 28, duration 19, epsilon 0.4040 done in 0.13407063484191895\n",
      "Episode 29, duration 97, epsilon 0.3713 done in 0.6783380508422852\n",
      "Episode 30, duration 66, epsilon 0.3507 done in 0.46649599075317383\n",
      "Episode 31, duration 77, epsilon 0.3284 done in 0.5489799976348877\n",
      "Episode 32, duration 69, epsilon 0.3099 done in 0.4903831481933594\n",
      "Episode 33, duration 32, epsilon 0.3017 done in 0.2216479778289795\n",
      "Episode 34, duration 171, epsilon 0.2621 done in 1.2358267307281494\n",
      "Episode 35, duration 200, epsilon 0.2237 done in 1.4346177577972412\n",
      "Episode 36, duration 99, epsilon 0.2073 done in 0.7724318504333496\n",
      "Episode 37, duration 85, epsilon 0.1945 done in 0.6694850921630859\n",
      "Episode 38, duration 98, epsilon 0.1810 done in 0.7284202575683594\n",
      "Episode 39, duration 33, epsilon 0.1768 done in 0.23238110542297363\n",
      "Episode 40, duration 150, epsilon 0.1591 done in 1.0725419521331787\n",
      "Episode 41, duration 122, epsilon 0.1466 done in 0.8600540161132812\n",
      "Episode 42, duration 145, epsilon 0.1335 done in 1.0451977252960205\n",
      "Episode 43, duration 145, epsilon 0.1223 done in 1.025223731994629\n",
      "Episode 44, duration 181, epsilon 0.1103 done in 1.3045318126678467\n",
      "Episode 45, duration 200, epsilon 0.0994 done in 1.4689621925354004\n",
      "Episode 46, duration 169, epsilon 0.0917 done in 1.215446949005127\n",
      "Episode 47, duration 112, epsilon 0.0873 done in 0.8569858074188232\n",
      "Episode 48, duration 128, epsilon 0.0828 done in 0.9994609355926514\n",
      "Episode 49, duration 142, epsilon 0.0785 done in 1.024963140487671\n",
      "Episode 50, duration 94, epsilon 0.0759 done in 0.6853530406951904\n",
      "Episode 51, duration 147, epsilon 0.0724 done in 1.13254976272583\n",
      "Episode 52, duration 134, epsilon 0.0696 done in 0.9670989513397217\n",
      "Episode 53, duration 124, epsilon 0.0673 done in 0.9779808521270752\n",
      "Episode 54, duration 132, epsilon 0.0651 done in 0.9593520164489746\n",
      "Episode 55, duration 172, epsilon 0.0627 done in 1.2307531833648682\n",
      "Episode 56, duration 200, epsilon 0.0604 done in 1.4394450187683105\n",
      "Episode 57, duration 124, epsilon 0.0592 done in 0.8808188438415527\n",
      "Episode 58, duration 110, epsilon 0.0583 done in 0.789360761642456\n",
      "Episode 59, duration 185, epsilon 0.0569 done in 1.3220219612121582\n",
      "Episode 60, duration 200, epsilon 0.0556 done in 1.4408042430877686\n",
      "Episode 61, duration 197, epsilon 0.0546 done in 1.4244170188903809\n",
      "Episode 62, duration 200, epsilon 0.0538 done in 1.57914400100708\n",
      "Episode 63, duration 200, epsilon 0.0531 done in 1.5479578971862793\n",
      "Episode 64, duration 200, epsilon 0.0525 done in 1.5601019859313965\n",
      "Episode 65, duration 200, epsilon 0.0521 done in 1.4699647426605225\n",
      "Episode 66, duration 185, epsilon 0.0517 done in 1.3218350410461426\n",
      "Episode 67, duration 200, epsilon 0.0514 done in 1.432084083557129\n",
      "Episode 68, duration 200, epsilon 0.0512 done in 1.4290270805358887\n",
      "Episode 69, duration 200, epsilon 0.0509 done in 1.4236969947814941\n",
      "Episode 70, duration 200, epsilon 0.0508 done in 1.4340872764587402\n",
      "Episode 71, duration 200, epsilon 0.0506 done in 1.4264419078826904\n",
      "Episode 72, duration 200, epsilon 0.0505 done in 1.4084839820861816\n",
      "Episode 73, duration 200, epsilon 0.0504 done in 1.425149917602539\n",
      "Episode 74, duration 200, epsilon 0.0503 done in 1.4195556640625\n",
      "Episode 75, duration 200, epsilon 0.0503 done in 1.5086240768432617\n",
      "Episode 76, duration 153, epsilon 0.0502 done in 1.1815910339355469\n",
      "Episode 77, duration 134, epsilon 0.0502 done in 0.9504120349884033\n",
      "Episode 78, duration 149, epsilon 0.0502 done in 1.0527191162109375\n",
      "Episode 79, duration 133, epsilon 0.0502 done in 0.9670040607452393\n",
      "Episode 80, duration 163, epsilon 0.0501 done in 1.1540961265563965\n",
      "Episode 81, duration 183, epsilon 0.0501 done in 1.5277228355407715\n",
      "Episode 82, duration 200, epsilon 0.0501 done in 1.5592758655548096\n",
      "Episode 83, duration 200, epsilon 0.0501 done in 1.6994378566741943\n",
      "Episode 84, duration 200, epsilon 0.0501 done in 1.5324609279632568\n",
      "Episode 85, duration 195, epsilon 0.0501 done in 1.5022058486938477\n",
      "Episode 86, duration 200, epsilon 0.0500 done in 1.50803804397583\n",
      "Episode 87, duration 200, epsilon 0.0500 done in 1.4120278358459473\n",
      "Episode 88, duration 200, epsilon 0.0500 done in 1.4136240482330322\n",
      "Episode 89, duration 200, epsilon 0.0500 done in 1.4045979976654053\n",
      "Episode 90, duration 200, epsilon 0.0500 done in 1.4260070323944092\n",
      "Episode 91, duration 200, epsilon 0.0500 done in 1.4576070308685303\n",
      "Episode 92, duration 189, epsilon 0.0500 done in 1.3520359992980957\n",
      "Episode 93, duration 200, epsilon 0.0500 done in 1.4377899169921875\n",
      "Episode 94, duration 200, epsilon 0.0500 done in 1.4539191722869873\n",
      "Episode 95, duration 177, epsilon 0.0500 done in 1.2707390785217285\n",
      "Episode 96, duration 200, epsilon 0.0500 done in 1.4261181354522705\n",
      "Episode 97, duration 149, epsilon 0.0500 done in 1.1107890605926514\n",
      "Episode 98, duration 200, epsilon 0.0500 done in 1.5988471508026123\n",
      "Episode 99, duration 200, epsilon 0.0500 done in 1.4292500019073486\n",
      "Episode 100, duration 200, epsilon 0.0500 done in 1.4310898780822754\n",
      "Episode 101, duration 200, epsilon 0.0500 done in 1.4971389770507812\n",
      "Episode 102, duration 200, epsilon 0.0500 done in 1.4208710193634033\n",
      "Episode 103, duration 200, epsilon 0.0500 done in 1.448422908782959\n",
      "Episode 104, duration 187, epsilon 0.0500 done in 1.3631823062896729\n",
      "Episode 105, duration 200, epsilon 0.0500 done in 1.4468138217926025\n",
      "Episode 106, duration 200, epsilon 0.0500 done in 1.4912230968475342\n",
      "Episode 107, duration 200, epsilon 0.0500 done in 1.5539767742156982\n",
      "Episode 108, duration 185, epsilon 0.0500 done in 1.3158619403839111\n",
      "Episode 109, duration 147, epsilon 0.0500 done in 1.0280711650848389\n",
      "Episode 110, duration 194, epsilon 0.0500 done in 1.3896780014038086\n",
      "Episode 111, duration 172, epsilon 0.0500 done in 1.219379186630249\n",
      "Episode 112, duration 200, epsilon 0.0500 done in 1.4590158462524414\n",
      "Episode 113, duration 200, epsilon 0.0500 done in 1.6554269790649414\n",
      "Episode 114, duration 197, epsilon 0.0500 done in 1.5629470348358154\n",
      "Episode 115, duration 194, epsilon 0.0500 done in 1.384166955947876\n",
      "Episode 116, duration 191, epsilon 0.0500 done in 1.3482167720794678\n",
      "Episode 117, duration 179, epsilon 0.0500 done in 1.282548189163208\n",
      "Episode 118, duration 188, epsilon 0.0500 done in 1.3578600883483887\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQNAgent(env, config)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 120\u001b[0m, in \u001b[0;36mDQNAgent.train\u001b[0;34m(self, num_episodes)\u001b[0m\n\u001b[1;32m    117\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(next_state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Perform one step of the optimization (on the policy network)\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Update of the target network's weights\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_done \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_update_period\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[27], line 82\u001b[0m, in \u001b[0;36mDQNAgent.update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Optimize the model\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 82\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# In-place gradient clipping\u001b[39;00m\n\u001b[1;32m     85\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_value_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_net\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[0;32m~/Teaching/DeepReinforcementLearning/.venv/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Teaching/DeepReinforcementLearning/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Teaching/DeepReinforcementLearning/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "config = {}\n",
    "config['nb_hidden'] = 128 # number of hidden neurons in each layer\n",
    "config['batch_size'] = 128 # number of transitions sampled from the replay buffer\n",
    "config['gamma'] = 0.99 # discount factor\n",
    "config['eps_start'] = 0.9 # starting value of epsilon\n",
    "config['eps_end'] = 0.05 # final value of epsilon\n",
    "config['eps_decay'] = 1000 # rate of exponential decay of epsilon, higher means a slower decay\n",
    "config['tau'] = 0.005 # update rate of the target network\n",
    "config['learning_rate'] = 1e-3 # learning rate of the optimizer\n",
    "config['target_update_period'] = 120 # update period (in steps) of the target network\n",
    "config['buffer_limit'] = 10000 # maximum number of transitions in the replay buffer\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Create the agent\n",
    "agent = DQNAgent(env, config)\n",
    "\n",
    "# Train the agent\n",
    "agent.train(num_episodes=250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43magent\u001b[49m\u001b[38;5;241m.\u001b[39mepisode_durations)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisodes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(agent.episode_durations)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
