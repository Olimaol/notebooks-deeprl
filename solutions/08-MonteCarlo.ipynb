{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20e5e5b1",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Olimaol/notebooks-deeprl/blob/main/solutions/08-MonteCarlo.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte-Carlo control "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install -U gymnasium pygame swig\n",
    "    !pip install moviepy==1.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rng = np.random.default_rng()\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import gymnasium as gym\n",
    "print(\"gym version:\", gym.__version__)\n",
    "\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "from IPython.display import Image, display\n",
    "\n",
    "class GymRecorder(object):\n",
    "    \"\"\"\n",
    "    Simple wrapper over moviepy to generate a .gif with the frames of a gym environment.\n",
    "    \n",
    "    The environment must have the render_mode `rgb_array_list`.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self._frames = []\n",
    "\n",
    "    def record(self, frames):\n",
    "        \"To be called at the end of an episode.\"\n",
    "        for frame in frames:\n",
    "            self._frames.append(np.array(frame))\n",
    "\n",
    "    def make_video(self, filename, show=True):\n",
    "        \"Generates the gif video.\"\n",
    "        directory = os.path.dirname(os.path.abspath(filename))\n",
    "        if not os.path.exists(directory):\n",
    "            os.mkdir(directory)\n",
    "        self.clip = ImageSequenceClip(list(self._frames), fps=self.env.metadata[\"render_fps\"])\n",
    "        self.clip.write_gif(filename, fps=self.env.metadata[\"render_fps\"], loop=0)\n",
    "        del self._frames\n",
    "        self._frames = []\n",
    "        if show:\n",
    "            display(Image(filename=filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The taxi environment\n",
    "\n",
    "In this exercise, we are going to apply **on-policy Monte-Carlo control** on the Taxi environment available in gym:\n",
    "\n",
    "<https://gymnasium.farama.org/environments/toy_text/taxi/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the environment in ansi mode, initialize it, and render the first state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode='ansi')\n",
    "state, info = env.reset()\n",
    "print(state)\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent is the black square. It can move up, down, left or right if there is no wall (the pipes and dashes). Its goal is to pick clients at the blue location and drop them off at the purple location. These locations are fixed (R, G, B, Y), but which one is the pick-up location and which one is the drop-off destination changes between each episode.\n",
    "\n",
    "**Q:** Re-run the previous cell multiple times to observe the diversity of initial states.\n",
    "\n",
    "The following cell prints the action space of the environment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Action Space:\", env.action_space)\n",
    "print(\"Number of actions:\", env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 6 discrete actions: south, north, east, west, pickup, dropoff.\n",
    "    \n",
    "Let's now look at the observation space (state space):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"State Space:\", env.observation_space)\n",
    "print(\"Number of states:\", env.observation_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 500 discrete states. What are they?\n",
    "\n",
    "* The taxi can be anywhere in the 5x5 grid, giving 25 different locations.\n",
    "* The passenger can be at any of the four locations R, G, B, Y or in the taxi: 5 values.\n",
    "* The destination can be any of the four locations: 4 values.\n",
    "\n",
    "This gives indeed 25x5x4 = 500 different combinations.\n",
    "\n",
    "The internal representation of a state is a number between 0 and 499. You can use the `encode` and `decode` methods of the environment to relate it to the state variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.unwrapped.encode(2, 1, 1, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "state = env.unwrapped.decode(328) \n",
    "print(\"State:\", list(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reward function is simple:\n",
    "\n",
    "* r = 20 when delivering the client at the correct location.\n",
    "* r = -10 when picking or dropping a client illegally (picking where there is no client, dropping a client somewhere else, etc)\n",
    "* r = -1 for all other transitions in order to incent the agent to be as fast as possible.\n",
    "\n",
    "The actions pickup and dropoff are very dangerous: take them at the wrong time and your return will be very low. The navigation actions are less critical.\n",
    "\n",
    "Depending on the initial state, the taxi will need at least 10 steps to deliver the client, so the maximal return you can expect is around 10 (+20 for the success, -1 for all the steps). \n",
    "\n",
    "The task is episodic: if you have not delivered the client within 200 steps, the episode stops (no particular reward)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random agent\n",
    "\n",
    "Let's now define a random agent that just samples the action space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Modify the random agent of last time, so that it accepts the `GymRecorder` that generates the .gif file.\n",
    "\n",
    "```python\n",
    "def train(self, nb_episodes, recorder=None):\n",
    "```\n",
    "\n",
    "The environment should be started in 'rgb_array_list' mode, not 'ansi'. The game looks different but has the same rules.\n",
    "\n",
    "```python\n",
    "env = gym.make(\"Taxi-v3\", render_mode='rgb_array_list')\n",
    "recorder = GymRecorder(env)\n",
    "```\n",
    "\n",
    "As episodes in Taxi can be quite long, only the last episode should be recorded:\n",
    "\n",
    "```python\n",
    "if recorder is not None and episode == nb_episodes -1:\n",
    "    recorder.record(self.env.render())\n",
    "```\n",
    "\n",
    "Perform 10 episodes, plot the obtained returns and vidualize the last episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "class RandomAgent:\n",
    "    \"\"\"\n",
    "    Random agent exploring uniformly the environment.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"Returns a random action by sampling the action space.\"\n",
    "        return self.env.action_space.sample()\n",
    "    \n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"Updates the agent using the transition (s, a, r, s').\"\n",
    "        pass\n",
    "    \n",
    "    def train(self, nb_episodes, recorder=None):\n",
    "        \"Runs the agent on the environment for nb_episodes. Returns the list of obtained rewards.\"\n",
    "        # List of returns\n",
    "        returns = []\n",
    "\n",
    "        for episode in range(nb_episodes):\n",
    "\n",
    "            # Sample the initial state\n",
    "            state, info = self.env.reset()\n",
    "\n",
    "            return_episode = 0.0\n",
    "            done = False\n",
    "            while not done:\n",
    "\n",
    "                # Select an action randomly\n",
    "                action = self.env.action_space.sample()\n",
    "                \n",
    "                # Sample a single transition\n",
    "                next_state, reward, terminal, truncated, info = self.env.step(action)\n",
    "                \n",
    "                # Go in the next state\n",
    "                state = next_state\n",
    "\n",
    "                # Update return\n",
    "                return_episode += reward\n",
    "\n",
    "                # End of the episode\n",
    "                done = terminal or truncated\n",
    "\n",
    "            # Record at the end of the episode\n",
    "            if recorder is not None and episode == nb_episodes -1:\n",
    "                recorder.record(self.env.render())\n",
    "            \n",
    "            # Append return\n",
    "            returns.append(return_episode)\n",
    "\n",
    "        return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "# Create the environment\n",
    "env = gym.make(\"Taxi-v3\", render_mode='rgb_array_list')\n",
    "recorder = GymRecorder(env)\n",
    "\n",
    "# Create the agent\n",
    "agent = RandomAgent(env)\n",
    "\n",
    "# Train for 10 episodes\n",
    "returns = agent.train(10, recorder)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(returns)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.show()\n",
    "\n",
    "recorder.make_video(\"videos/taxi.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** What do you think of the returns obtained by the random agent? Conclude on the difficulty of the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** The optimal returns are around 10, but the obtained returns with a random policy are very negative (-700, mostly due to many illegal pickups or dropoffs). One can expect a very huge variance of the returns, so learning will be slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On-policy Monte-Carlo control\n",
    "\n",
    "Now let's apply on-policy MC control on the Taxi environment. As a reminder, here the meta-algorithm:\n",
    "\n",
    "* **while** True:\n",
    "\n",
    "    1. Generate an episode $\\tau = (s_0, a_0, r_1, \\ldots, s_T)$ using the current **stochastic** policy $\\pi$.\n",
    "\n",
    "    2. For each state-action pair $(s_t, a_t)$ in the episode, update the estimated Q-value:\n",
    "\n",
    "    $$\n",
    "        Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\, (R_t - Q(s_t, a_t))\n",
    "    $$\n",
    "\n",
    "    3. For each state $s_t$ in the episode, improve the policy (e.g. $\\epsilon$-greedy):\n",
    "\n",
    "    $$\n",
    "        \\pi(s_t, a) = \\begin{cases}\n",
    "                        1 - \\epsilon \\; \\text{if} \\; a = a^* \\\\\n",
    "                        \\frac{\\epsilon}{|\\mathcal{A(s_t)}|-1} \\; \\text{otherwise.} \\\\\n",
    "                      \\end{cases}\n",
    "    $$\n",
    "    \n",
    "In practice, we will need:\n",
    "\n",
    "* a **Q-table** storing the estimated Q-value of each state-action pair: its size will be (500, 6).\n",
    "\n",
    "* an $\\epsilon$-greedy action selection to select actions in the current state.\n",
    "\n",
    "* an learning mechanism allowing to update the Q-value of all state-action pairs encountered in the episode.\n",
    "\n",
    "**Q:** Create a `MonteCarloAgent` class implementing on-policy MC for the Taxi environment. \n",
    "\n",
    "Use $\\gamma = 0.9$, $\\epsilon = 0.1$ and $\\alpha=0.01$ (pass these parameters to the constructor of the agent and store them). Train the agent for 20000 episodes (yes, 20000... Start with one episode to debug everything and then launch the simulation. It should take around one minute). Save the return of each episode in a list, as well as the number of steps of the episode, and plot them in the end. \n",
    "\n",
    "The environment should be created without rendering (`env = gym.make(\"Taxi-v3\")`, no recorder).\n",
    "\n",
    "Implementing the action selection mechanism should not be a problem, it is the same as for bandits. Little trick (not obligatory): you can implement $\\epsilon$-greedy as:\n",
    "\n",
    "```python\n",
    "action = self.Q[state, :].argmax()\n",
    "if rng.random() < epsilon:\n",
    "    action = self.env.action_space.sample()\n",
    "```\n",
    "\n",
    "This is not exactly $\\epsilon$-greedy, as `env.action_space.sample()` may select the greedy action again. In practice it does not matter, it only changes the meaning of $\\epsilon$, but the action selection stays similar. It is better to rely on `env.action_space.sample()` for the exploration, as some Gym problem work better with a normal distribution for the exploration than with uniform (e.g. continuous problems). \n",
    "\n",
    "Do not select the greedy action with `self.Q[state, :].argmax()` but `rng.random.choice(np.where(self.Q[state, :] == self.Q[state, :].max())[0])`: at the beginning of learning, where the Q-values are all 0, you would otherwise always take the first action (south).\n",
    "\n",
    "The `update()` method should take a complete episode as argument, using a list of (state, action, reward) transitions. It should be called at the end of an episode only, not after every step.\n",
    "\n",
    "A bit tricky is the calculation of the returns for each visited state. The naive approach would look like:\n",
    "\n",
    "```python\n",
    "T = len(episode)\n",
    "for t in range(T):\n",
    "    state, action, reward = episode[t]\n",
    "    return_state = 0.0\n",
    "    for k in range(t, T): # rewards coming after t\n",
    "        next_state, next_action, next_reward = episode[k]\n",
    "        return_state += gamma**k * reward\n",
    "    self.Q[state, action] += alpha * (return_state - self.Q[state, action])\n",
    "```\n",
    "\n",
    "The double for loop can be computationally expensive for long episodes (complexity T log T). It is much more efficient to iterate **backwards** on the episode, starting from the last transition and iterating until the first one, and using the fact that:\n",
    "\n",
    "$$R_{t} = r_{t+1} + \\gamma \\, R_{t+1}$$\n",
    "\n",
    "The terminal state $s_T$ has a return of 0 by definition. The last transition $s_{T-1} \\rightarrow s_{T}$ has therefore a return of $R_{T-1} = r_T$. The transition before that has a return of $R_{T-2} = r_{T-1}  + \\gamma \\, R_{T-1}$, and so on. You can then compute the returns of each action taken in the episode (and update its Q-value) in **linear time**.\n",
    "\n",
    "To iterate backwards over the list of transitions, use the `reversed()` operator:\n",
    "\n",
    "```python\n",
    "l = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "for a in reversed(l):\n",
    "    print(a)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "class MonteCarloAgent:\n",
    "    \"\"\"\n",
    "    Online Monte-Carlo agent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, gamma, epsilon, alpha):\n",
    "        \"\"\"\n",
    "        :param env: gym-like environment\n",
    "        :param gamma: discount factor\n",
    "        :param epsilon: exploration parameter\n",
    "        :param alpha: learning rate\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Q_table\n",
    "        self.Q = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"Returns an action using epsilon-greedy action selection.\"\n",
    "        \n",
    "        action = rng.choice(np.where(self.Q[state, :] == self.Q[state, :].max())[0])\n",
    "        \n",
    "        if rng.random() < self.epsilon:\n",
    "            action = self.env.action_space.sample() \n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def update(self, episode):\n",
    "        \"Updates the agent using a complete episode.\"\n",
    "        # Terminal states have a return of 0\n",
    "        return_episode = 0.0\n",
    "        \n",
    "        # Iterate backwards over the episode\n",
    "        for state, action, reward in reversed(episode):\n",
    "            \n",
    "            # Compute the return\n",
    "            return_episode = reward + self.gamma * return_episode\n",
    "            \n",
    "            # Update the Q-value\n",
    "            self.Q[state, action] += self.alpha * (return_episode - self.Q[state, action])\n",
    "            \n",
    "    \n",
    "    def train(self, nb_episodes, recorder=False):\n",
    "        \"\"\"\n",
    "        Runs the agent on the environment for nb_episodes. Returns the list of obtained returns and the number of steps.\n",
    "        \"\"\"\n",
    "\n",
    "        # Returns and steps\n",
    "        returns = []\n",
    "        steps = []\n",
    "\n",
    "        # Fixed number of episodes\n",
    "        for episode in range(nb_episodes):\n",
    "\n",
    "            # Reset\n",
    "            state, info = self.env.reset()\n",
    "            return_episode = 0.\n",
    "            nb_steps = 0\n",
    "\n",
    "            # Store transitions\n",
    "            transitions = []\n",
    "\n",
    "            # Sample the episode\n",
    "            done = False\n",
    "            while not done:\n",
    "\n",
    "                # Select an action \n",
    "                action = self.act(state)\n",
    "\n",
    "                # Perform the action\n",
    "                next_state, reward, terminal, truncated, info = self.env.step(action)\n",
    "\n",
    "                # Store the transition\n",
    "                transitions.append([state, action, reward])\n",
    "\n",
    "                # Go in the next state\n",
    "                state = next_state\n",
    "\n",
    "                # Terminal state\n",
    "                done = terminal or truncated\n",
    "\n",
    "                # Increment time\n",
    "                nb_steps += 1\n",
    "                return_episode += reward\n",
    "\n",
    "            # Update the Monte Carlo agent after the episode is completed\n",
    "            self.update(transitions)    \n",
    "\n",
    "            # Store info\n",
    "            returns.append(return_episode)\n",
    "            steps.append(nb_steps)\n",
    "            \n",
    "            \n",
    "        return returns, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "# Parameters\n",
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "alpha = 0.01\n",
    "nb_episodes = 20000\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "# Create the agent\n",
    "agent = MonteCarloAgent(env, gamma, epsilon, alpha)\n",
    "\n",
    "# Train the agent \n",
    "returns, steps = agent.train(nb_episodes)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(121)\n",
    "plt.plot(returns)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Returns\")\n",
    "plt.subplot(122)\n",
    "plt.plot(steps)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may observe, the returns have a huge variance due to the exploration, what makes the plot quite ugly and unreadable. The following function allows to smooth the returns using a sliding average over the last $N$ epochs. Note that the first values will be off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_average(x, N):\n",
    "    kernel = np.ones(N) / N\n",
    "    return np.convolve(x, kernel, mode='same')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Plot the returns and steps, as well as their sliding average. Comment on the influence of exploration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(121)\n",
    "plt.plot(returns)\n",
    "plt.plot(running_average(returns, 1000))\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Returns\")\n",
    "plt.subplot(122)\n",
    "plt.plot(steps)\n",
    "plt.plot(running_average(steps, 1000))\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Extend the `MonteCarloAgent` class with a test method that performs a single episode on the environment **without exploration**, optionally records the episode but does **not** learn. \n",
    "\n",
    "```python\n",
    "class MonteCarloAgentTest (MonteCarloAgent):\n",
    "    \"\"\"\n",
    "    Online Monte-Carlo agent with a test method.\n",
    "    \"\"\"\n",
    "\n",
    "    def test(self, recorder=None):\n",
    "        # ...\n",
    "```\n",
    "\n",
    "In the test method, backup the previous value of `epsilon` in a temporary variable and reset it at the end of the episode. Have the method return the undiscounted return of the episode, as well as the number of steps until termination.\n",
    "\n",
    "\n",
    "Perform 1000 test episodes without rendering and report the mean return over these 1000 episodes as the final performance of your agent.\n",
    "\n",
    "*Tip:* To avoid re-training the agent, simply transfer the Q-table from the previous agent:\n",
    "\n",
    "```python\n",
    "test_agent = MonteCarloAgentTest(env, gamma, epsilon, alpha)\n",
    "test_agent.Q = agent.Q\n",
    "\n",
    "return_episode, nb_steps = test_agent.test()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "class MonteCarloAgentTest (MonteCarloAgent):\n",
    "    \"\"\"\n",
    "    Online Monte-Carlo agent with a test method.\n",
    "    \"\"\"\n",
    "\n",
    "    def test(self, recorder=None):\n",
    "        \"Performs a test episode without exploration.\"\n",
    "        # Set epsilon to 0\n",
    "        previous_epsilon = self.epsilon\n",
    "        self.epsilon = 0.0\n",
    "        \n",
    "        # Reset\n",
    "        state, info = self.env.reset()\n",
    "        done = False\n",
    "        nb_steps = 0\n",
    "        return_episode= 0\n",
    "\n",
    "        # Sample the episode\n",
    "        while not done:\n",
    "            action = self.act(state)\n",
    "            next_state, reward, terminal, truncated, info = self.env.step(action)\n",
    "            return_episode += reward\n",
    "            state = next_state\n",
    "            done = terminal or truncated\n",
    "            nb_steps += 1\n",
    "\n",
    "        if recorder is not None:\n",
    "            recorder.record(self.env.render())\n",
    "\n",
    "        # Restore epsilon\n",
    "        self.epsilon = previous_epsilon\n",
    "            \n",
    "        return return_episode, nb_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "# Parameters\n",
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "alpha = 0.01\n",
    "nb_episodes = 20000\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "# Create the agent\n",
    "test_agent = MonteCarloAgentTest(env, gamma, epsilon, alpha)\n",
    "test_agent.Q = agent.Q\n",
    "\n",
    "\n",
    "# Test the agent for 1000 episodes\n",
    "test_returns = []\n",
    "test_steps = []\n",
    "for episode in range(1000):\n",
    "    return_episode, nb_steps = test_agent.test()\n",
    "    test_returns.append(return_episode)\n",
    "    test_steps.append(nb_steps)\n",
    "print(\"Test performance\", np.mean(test_returns))\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(121)\n",
    "plt.hist(test_returns)\n",
    "plt.xlabel(\"Returns\")\n",
    "plt.subplot(122)\n",
    "plt.hist(test_steps)\n",
    "plt.xlabel(\"Number of steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Visualize one episode after training. The environment used for training had no render mode, but you can always create a new environment and set it in the agent:\n",
    "\n",
    "```python\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\") # or rgb_array_list\n",
    "test_agent.env = env\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array_list\")\n",
    "recorder = GymRecorder(env)\n",
    "test_agent.env = env\n",
    "\n",
    "return_episode, nb_steps = test_agent.test(recorder)\n",
    "\n",
    "recorder.make_video(\"videos/taxi-trained.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** The agent successfully picks and delivers the client at the correct location. The path is not always the shortest one (especially in the vast area in the top right), but that is fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "### Early stopping\n",
    "\n",
    "**Q:** Train the agent for the smallest number of episodes where the returns seem to have stabilized (e.g. 2000 episodes). Test the agent. Does it work? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "# Parameters\n",
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "alpha = 0.01\n",
    "nb_episodes = 2000\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "# Create the agent\n",
    "agent = MonteCarloAgentTest(env, gamma, epsilon, alpha)\n",
    "\n",
    "# Train the agent\n",
    "returns, steps = agent.train(nb_episodes)\n",
    "\n",
    "# Plot training returns\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(121)\n",
    "plt.plot(returns)\n",
    "plt.plot(running_average(returns, 100))\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Returns\")\n",
    "plt.subplot(122)\n",
    "plt.plot(steps)\n",
    "plt.plot(running_average(steps, 100))\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "# Test the agent for 1000 episodes\n",
    "test_returns = []\n",
    "test_steps = []\n",
    "for episode in range(1000):\n",
    "    return_episode, nb_steps = agent.test()\n",
    "    test_returns.append(return_episode)\n",
    "    test_steps.append(nb_steps)\n",
    "print(\"Test performance\", np.mean(test_returns))\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(121)\n",
    "plt.hist(test_returns)\n",
    "plt.xlabel(\"Returns\")\n",
    "plt.subplot(122)\n",
    "plt.hist(test_steps)\n",
    "plt.xlabel(\"Number of steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** Although the returns are already between -20 and +10, as after 50000 episodes, the greedy policy is not optimal as some states still have a very bad policy (the agent sometimes goes back and forth between two states) or just a bad one (not optimal path). The negative returns at the end of learning are due to the exploration: the agent performs illegal pickups or dropoffs. Early in learning, this is due to bad estimates of the Q-values. The return of an episode is a bad estimate of the performance: it report both the exploration and the exploitation. Unfortunately, it is the only one we have...  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discount rate\n",
    "\n",
    "**Q:** Change the value of the discount factor $\\gamma$. As the task is episodic (maximum 200 steps), try a discount rate of 1. What happens? Conclude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "# Parameters\n",
    "gamma = 1.0\n",
    "epsilon = 0.1\n",
    "alpha = 0.01\n",
    "nb_episodes = 20000\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "# Create the agent\n",
    "agent = MonteCarloAgentTest(env, gamma, epsilon, alpha)\n",
    "\n",
    "# Train the agent\n",
    "returns, steps = agent.train(nb_episodes)\n",
    "\n",
    "# Plot training returns\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(121)\n",
    "plt.plot(returns)\n",
    "plt.plot(running_average(returns, 100))\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Returns\")\n",
    "plt.subplot(122)\n",
    "plt.plot(steps)\n",
    "plt.plot(running_average(steps, 100))\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"steps\")\n",
    "plt.show()\n",
    "\n",
    "# Test the agent for 1000 episodes\n",
    "test_returns = []\n",
    "test_steps = []\n",
    "for episode in range(1000):\n",
    "    return_episode, nb_steps = agent.test()\n",
    "    test_returns.append(return_episode)\n",
    "    test_steps.append(nb_steps)\n",
    "print(\"Test performance\", np.mean(test_returns))\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(121)\n",
    "plt.hist(test_returns)\n",
    "plt.xlabel(\"Returns\")\n",
    "plt.subplot(122)\n",
    "plt.hist(test_steps)\n",
    "plt.xlabel(\"Number of steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** Rather tricky question... With a discount factor of 1, the agent does not converge as fast as with gamma = 0.9. This is due to the **variance** of the returns: imagine your episode is optimal all along, but at the last moment, the agent performs an illegal dropoff action. The undiscounted return of the episode will be negative and **all** actions taken during the episode will be punished, although only the last one is responsible for the bad return.\n",
    "\n",
    "Using a discount factor < 1 allows the first actions to stop caring about the final rewards, as they are discounted by $\\gamma^T$, which is very small. Take-home message: even if your task is episodic, use $\\gamma < 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate\n",
    "\n",
    "**Q:** Vary the learning rate `alpha`. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "# Parameters\n",
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "alpha = 0.5\n",
    "nb_episodes = 20000\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "# Create the agent\n",
    "agent = MonteCarloAgentTest(env, gamma, epsilon, alpha)\n",
    "\n",
    "# Train the agent\n",
    "returns, steps = agent.train(nb_episodes)\n",
    "\n",
    "# Plot training returns\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(121)\n",
    "plt.plot(returns)\n",
    "plt.plot(running_average(returns, 100))\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Returns\")\n",
    "plt.subplot(122)\n",
    "plt.plot(steps)\n",
    "plt.plot(running_average(steps, 100))\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "# Test the agent for 1000 episodes\n",
    "test_returns = []\n",
    "test_steps = []\n",
    "for episode in range(1000):\n",
    "    return_episode, nb_steps = agent.test()\n",
    "    test_returns.append(return_episode)\n",
    "    test_steps.append(nb_steps)\n",
    "print(\"Test performance\", np.mean(test_returns))\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(121)\n",
    "plt.hist(test_returns)\n",
    "plt.xlabel(\"Returns\")\n",
    "plt.subplot(122)\n",
    "plt.hist(test_steps)\n",
    "plt.xlabel(\"Number of steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** If the learning rate is too high (0.5), the network does not converge and becomes unstable, as updates \"erases\" very quickly the previous values, what is bad given the high variance of the returns. If the learning rate is too low, learning takes forever. Classical machine learning problem... 0.01 works actually quite well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration parameter\n",
    "\n",
    "**Q:** Vary the exploration parameter `epsilon` and observe its impact on learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "# Parameters\n",
    "gamma = 0.9\n",
    "epsilon = 0.01 # or 0.3, etc.\n",
    "alpha = 0.01\n",
    "nb_episodes = 20000\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "# Create the agent\n",
    "agent = MonteCarloAgentTest(env, gamma, epsilon, alpha)\n",
    "\n",
    "# Train the agent\n",
    "returns, steps = agent.train(nb_episodes)\n",
    "\n",
    "# Plot training returns\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(121)\n",
    "plt.plot(returns)\n",
    "plt.plot(running_average(returns, 100))\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Returns\")\n",
    "plt.subplot(122)\n",
    "plt.plot(steps)\n",
    "plt.plot(running_average(steps, 100))\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "# Test the agent for 1000 episodes\n",
    "test_returns = []\n",
    "test_steps = []\n",
    "for episode in range(1000):\n",
    "    return_episode, nb_steps = agent.test()\n",
    "    test_returns.append(return_episode)\n",
    "    test_steps.append(nb_steps)\n",
    "print(\"Test performance\", np.mean(test_returns))\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(121)\n",
    "plt.hist(test_returns)\n",
    "plt.xlabel(\"Returns\")\n",
    "plt.subplot(122)\n",
    "plt.hist(test_steps)\n",
    "plt.xlabel(\"Number of steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** The exploration should be at least 0.1 in order to find the optimal policy. 0.2 or 0.3 find a good policy faster, but explore too much at the end of learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration scheduling\n",
    "\n",
    "Even with a good learning rate (0.01) and a discount factor of 0.9, the exploration parameter as a huge impact on the performance: too low and the agent does not find the optimal policy, too high and the agent is inefficient at the end of learning. \n",
    "\n",
    "**Q:** Implement scheduling for epsilon. You can use exponential scheduling as in the bandits exercise:\n",
    "\n",
    "$$\\epsilon = \\epsilon \\times (1 - \\epsilon_\\text{decay})$$\n",
    "\n",
    "at the end of each episode, with $\\epsilon_\\text{decay}$ being a small decay parameter (`1e-5` or so).\n",
    "\n",
    "Find a correct value for $\\epsilon_\\text{decay}$. Do not hesitate to fine-tune alpha at the same time.\n",
    "\n",
    "*Tip:* Prepare and visualize the scheduling in a different cell, and use the initial value of $\\epsilon$ and $\\epsilon_\\text{decay}$ that seem to make sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "class DecayMonteCarloAgent (MonteCarloAgentTest):\n",
    "    \"\"\"\n",
    "    Monte-Carlo agent with decay of the exploration parameter.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, gamma, epsilon, decay_epsilon, alpha):\n",
    "        \"\"\"\n",
    "        :param env: gym-like environment\n",
    "        :param gamma: discount factor\n",
    "        :param epsilon: exploration parameter\n",
    "        :param decay_epsilon: exploration decay parameter\n",
    "        :param alpha: learning rate\n",
    "        \"\"\"\n",
    "        self.decay_epsilon = decay_epsilon\n",
    "        \n",
    "        super().__init__(env, gamma, epsilon, alpha)\n",
    "    \n",
    "    \n",
    "    def train(self, nb_episodes, recorder=None):\n",
    "        \"Runs the agent on the environment for nb_episodes. Returns the list of obtained returns and steps.\"\n",
    "\n",
    "        # Returns\n",
    "        returns = []\n",
    "        steps = []\n",
    "\n",
    "        # Fixed number of episodes\n",
    "        for episode in range(nb_episodes):\n",
    "\n",
    "            # Reset\n",
    "            state, info = self.env.reset()\n",
    "            done = False\n",
    "            return_episode = 0.0\n",
    "            nb_steps = 0\n",
    "\n",
    "            # Store transitions\n",
    "            transitions = []\n",
    "\n",
    "            # Sample the episode\n",
    "            while not done:\n",
    "\n",
    "                # Select an action \n",
    "                action = self.act(state)\n",
    "\n",
    "                # Perform the action\n",
    "                next_state, reward, terminal, truncated, info = self.env.step(action)\n",
    "\n",
    "                # Store the transition\n",
    "                transitions.append([state, action, reward])\n",
    "\n",
    "                # Go in the next state\n",
    "                state = next_state\n",
    "\n",
    "                # Increment time\n",
    "                nb_steps += 1\n",
    "                return_episode += reward\n",
    "                \n",
    "                # Terminal state\n",
    "                done = terminal or truncated\n",
    "\n",
    "            # Update the Monte Carlo agent after the episode is completed\n",
    "            self.update(transitions)   \n",
    "            \n",
    "            # Decay epsilon\n",
    "            self.epsilon = self.epsilon * (1 - self.decay_epsilon)\n",
    "\n",
    "            # Store info\n",
    "            returns.append(return_episode)\n",
    "            steps.append(nb_steps)\n",
    "            \n",
    "            \n",
    "        return returns, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "# Parameters\n",
    "gamma = 0.9\n",
    "epsilon = 0.4\n",
    "decay_epsilon = 1e-4\n",
    "alpha = 0.05\n",
    "nb_episodes = 20000\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "# Create the agent\n",
    "agent = DecayMonteCarloAgent(env, gamma, epsilon, decay_epsilon, alpha)\n",
    "\n",
    "# Train the agent\n",
    "returns, steps = agent.train(nb_episodes)\n",
    "\n",
    "# Plot training returns\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(121)\n",
    "plt.plot(returns)\n",
    "plt.plot(running_average(returns, 100))\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Returns\")\n",
    "plt.subplot(122)\n",
    "plt.plot(steps)\n",
    "plt.plot(running_average(steps, 100))\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "# Test the agent for 1000 episodes\n",
    "test_returns = []\n",
    "test_steps = []\n",
    "for episode in range(1000):\n",
    "    return_episode, nb_steps = agent.test()\n",
    "    test_returns.append(return_episode)\n",
    "    test_steps.append(nb_steps)\n",
    "print(\"Test performance\", np.mean(test_returns))\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(121)\n",
    "plt.hist(test_returns)\n",
    "plt.xlabel(\"Returns\")\n",
    "plt.subplot(122)\n",
    "plt.hist(test_steps)\n",
    "plt.xlabel(\"Number of steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** As seen with bandits, decaying the exploration parameter with the right schedule improves very significantly the speed of learning and the optimality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeprl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
